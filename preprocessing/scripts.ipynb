{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import gpt as gpt\n",
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w', encoding='utf-8') as fp:\n",
    "        json.dump(data, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "# openai\n",
    "openai_api_key = open(\"openai_api_key\").read() \n",
    "openai_client=OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toString(segment):\n",
    "    result = ''\n",
    "    for dialogue in segment:\n",
    "        if dialogue['speaker'] == 'interviewer':\n",
    "            result += \"Interviewer: \"\n",
    "        else:\n",
    "            result += \"Interviewee: \"\n",
    "        result += dialogue['content'] + \"\\n\"\n",
    "    return result\n",
    "\n",
    "def extract_keywords(paragraph):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a keyword extraction system that extracts keywords from a monologue. \n",
    "            The monologue is about a person who is talking about their life.\n",
    "            The keywords should be the most important words in the monologue.\n",
    "            Use the exact words that the person uses in the monologue.\n",
    "            Replay with a list of keywords in the following JSON format:\n",
    "            {{\n",
    "                \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", ...]\n",
    "            }}\n",
    "            \"\"\"\n",
    "       },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": paragraph\n",
    "       }\n",
    "    ]\n",
    "    # keywords = request_chatgpt_gpt4(messages)\n",
    "    return messages\n",
    "def extract_title(conversation):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a title analysis system. \n",
    "            You are given a conversation between two people: Interviewer and Interviewee. \n",
    "            Give a concise title for the conversation with no more than 10 words.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": conversation\n",
    "        }\n",
    "    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# reformat\n",
    "for segmented_file in glob.glob(\"segmented_transcripts_more/*.json\"):\n",
    "    participant_id = segmented_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    segments = json.load(open(segmented_file))\n",
    "    reformat_segments = []\n",
    "    for segment_index, segment in enumerate(segments):\n",
    "        reformat_segments.append({\n",
    "            \"id\": \"{}_{}\".format(participant_id, segment_index),\n",
    "            \"conversation\": segment,\n",
    "        })\n",
    "    save_json(reformat_segments, 'tmp/segments_formatted/{}.json'.format(participant_id))\n",
    "\n",
    "all_chunks = []\n",
    "for interview_file in glob.glob(\"tmp/segments_formatted/*.json\"):\n",
    "    pid = interview_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    interview_data = json.load(open(interview_file))\n",
    "    print(interview_file)\n",
    "    keyword_prompts = []\n",
    "    title_prompts = []\n",
    "    for segment in interview_data:\n",
    "        interviewee_messages = \"\\n\".join([message['content'] for message in segment['conversation'] if message['speaker'] == \"interviewee\"])\n",
    "        segment_messages = toString(segment['conversation'])\n",
    "        keyword_prompts.append(extract_keywords(interviewee_messages))\n",
    "        title_prompts.append(extract_title(segment_messages))\n",
    "\n",
    "    segment_keywords = gpt.multithread_prompts(openai_client, keyword_prompts, temperature=0, response_format=\"json\")\n",
    "    segment_keywords = [json.loads(keyword)['keywords'] for keyword in segment_keywords]\n",
    "    segment_titles = gpt.multithread_prompts(openai_client, title_prompts, temperature=1.0)\n",
    "    result = []\n",
    "    for segment, keywords, title in zip(interview_data, segment_keywords, segment_titles):\n",
    "        segment_messages = toString(segment['conversation'])\n",
    "        segment['raw_keywords'] = keywords\n",
    "        segment['title'] = title\n",
    "        result.append(segment)\n",
    "        all_chunks.append(segment)\n",
    "    save_json(result, \"result/documents/{}.json\".format(pid))\n",
    "save_json(all_chunks, \"result/chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "for document in glob.glob(\"result/documents/*.json\"):\n",
    "    chunks = json.load(open(document))\n",
    "    all_chunks += chunks\n",
    "save_json(all_chunks, \"result/chunks.json\")\n",
    "len(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_background = \"\"\"Zaun is a heavily industrialized city characterized by unregulated technological development and alchemical innovation. \n",
    "Its rapid growth has resulted in significant environmental degradation, including severe air and water pollution. \n",
    "Emissions from factories and laboratories produce a persistent smog, known as the \"Gray Wind,\" leading to widespread respiratory issues among residents. \n",
    "The city's waterways are heavily contaminated with industrial waste, impacting aquatic ecosystems and public health. \n",
    "These environmental challenges highlight the urgent need for sustainable practices and effective regulatory frameworks to balance Zaun's industrial progress with environmental and human well-being.\n",
    "\"\"\"\n",
    "# generating explanations for keywords, the explanations are used to generated embeddings for the keywords\n",
    "def explain_prompt(keyword):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are a dictionary system.\n",
    "            The user wants to know the meaning of a specific word in the following context: {project_background}\n",
    "            Reply with the definition of the word.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Please explain {}\".format(keyword)\n",
    "        }\n",
    "    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "keyword_freq = defaultdict(int)\n",
    "for interview_file in glob.glob(\"result/documents/*.json\"):\n",
    "    print(interview_file)\n",
    "    interview_data = json.load(open(interview_file))\n",
    "    for segment in interview_data:\n",
    "        keywords = segment['raw_keywords']\n",
    "        for k in keywords:\n",
    "            keyword_freq[k] += 1\n",
    "\n",
    "# filter out keywords that only appear once\n",
    "all_keywords = [k for k in keyword_freq.keys() if keyword_freq[k] > 1]\n",
    "\n",
    "print(\"Number of keywords: \", len(all_keywords))\n",
    "# collect statistics\n",
    "keyword_statistics = [{\"keyword\": k, \"frequency\": keyword_freq[k]} for k in all_keywords]\n",
    "\n",
    "# get explanations\n",
    "explain_prompts = [explain_prompt(keyword) for keyword in all_keywords]\n",
    "\n",
    "# get embeddings from explanations\n",
    "explanations = gpt.multithread_prompts(openai_client, explain_prompts)\n",
    "embeddings = gpt.multithread_embeddings(openai_client, explanations)\n",
    "\n",
    "# save\n",
    "result = []\n",
    "for keyword, explanation, embedding in zip(all_keywords, explanations, embeddings):\n",
    "    result.append({\n",
    "        \"keyword\": keyword,\n",
    "        \"explanation\": explanation,\n",
    "        \"embeddings\": embedding\n",
    "    })\n",
    "save_json(result, \"keyword/keywords.json\")\n",
    "save_json(keyword_statistics, \"keyword/keyword_statistics.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyudao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
