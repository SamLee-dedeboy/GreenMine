{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import gpt as gpt\n",
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w', encoding='utf-8') as fp:\n",
    "        json.dump(data, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "# openai\n",
    "openai_api_key = open(\"openai_api_key\").read()\n",
    "openai_client=OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = \"\"\"\n",
    "Zaun is a heavily industrialized city characterized by unregulated technological development and alchemical innovation. \n",
    "Its rapid growth has resulted in significant environmental degradation, including severe air and water pollution. \n",
    "Emissions from factories and laboratories produce a persistent smog, known as the \"Gray Wind,\" leading to widespread respiratory issues among residents. \n",
    "The city's waterways are heavily contaminated with industrial waste, impacting aquatic ecosystems and public health. \n",
    "These environmental challenges highlight the urgent need for sustainable practices and effective regulatory frameworks to balance Zaun's industrial progress with environmental and human well-being.\n",
    "\"\"\"\n",
    "interviewer_description = \"\"\"\n",
    "An environmental scientist is conducting a community-based study in Zaun to identify actionable solutions to the cityâ€™s environmental issues. \n",
    "Through interviews with residents, the scientist aims to understand the lived experiences of those most affected by air and water pollution. \n",
    "The study focuses on gathering insights into health impacts, coping strategies, and potential mitigation measures. \n",
    "By integrating local knowledge with scientific analysis, the initiative seeks to develop practical and community-informed solutions to improve environmental and public health outcomes in Zaun.\n",
    "\"\"\"\n",
    "desired_segments = 10\n",
    "reply_format = \"\"\"\n",
    "{\n",
    "    transcript: [\n",
    "            {\n",
    "                \"speaker\": \"interviewer\",\n",
    "                \"content\": xxx,\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": \"interviewee\",\n",
    "                \"content\": xxx,\n",
    "            },\n",
    "            {\n",
    "                \"speaker \"interviewer\",\n",
    "                \"content\": xxx,\n",
    "            },\n",
    "            {\n",
    "                \"speaker\": \"interviewee\",\n",
    "                \"content\": xxx,\n",
    "            },\n",
    "            ...\n",
    "    ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate transcripts\n",
    "interviewees = json.load(open(\"interviewees.json\"))\n",
    "generation_prompts = []\n",
    "for interviewee in interviewees:\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are a transcript generator.  According to the user provided information, generate the transcript mimicking real-world conversations.\n",
    "            The interviewer should first ask about the interviewee's background, ask about their personal experiences, and then gradually move on to the main topic of the interview.\n",
    "            The generated transcript should have back-and-forth conversations between the interviewer and the interviewee.\n",
    "            The interviewer should occasionally ask follow-up questions or provide feedback to keep the conversation engaging.\n",
    "            Try to make the conversations natural and realistic, like they are transcribed from a recording.\n",
    "            You can be creative and add details based on the provided context.\n",
    "            There should be at least 70 turns of conversation in the transcript.\n",
    "            Reply with the following JSON format:\n",
    "            {reply_format} \n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Background of the interview: {background}\n",
    "            Description of the interviewer: {interviewer_description}\n",
    "            Description of the interviewee: {json.dumps(interviewee)}\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "    generation_prompts.append(prompt)\n",
    "responses = gpt.multithread_prompts(openai_client, generation_prompts, temperature=1.0, response_format=\"json\")\n",
    "responses = [json.loads(response)['transcript'] for response in responses]\n",
    "for response, interviewee in zip(responses, interviewees):\n",
    "    save_json(response, f\"transcripts/{interviewee['name']}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation\n",
    "transcript_str_generator = lambda t: \"\\n\".join([f\"{i} - {turn['speaker']}: {turn['content']}\" for i, turn in enumerate(t)])\n",
    "segmentation_prompt_generator = lambda transcript_str: [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a transcript segmenter. The user will give you a indexed transcript. \n",
    "        Your task is to divide the provided transcript into segments that are coherent and meaningful.\n",
    "        In each segment, the conversation should flow naturally and cover a specific topic or subtopic.\n",
    "        Reply with the start and end index of each segment with the following JSON format:\n",
    "        {\n",
    "            \"segments\": [\n",
    "                [start_index_1, end_index_1],\n",
    "                [start_index_2, end_index_2],\n",
    "                ...\n",
    "             ] \n",
    "        }\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": transcript_str\n",
    "    }\n",
    "]\n",
    "def segment_list(data, segment_indices):\n",
    "    segments = []\n",
    "    for segment_range in segment_indices:\n",
    "        start = segment_range[0]\n",
    "        end = segment_range[1]\n",
    "        segments.append(data[start:end+1])\n",
    "    return segments\n",
    "\n",
    "import glob\n",
    "segmentation_prompts = []\n",
    "interviewees = []\n",
    "transcripts = []\n",
    "for transcript_file in glob.glob(\"transcripts/*.json\"):\n",
    "    transcript = json.load(open(transcript_file))\n",
    "    transcript_str = transcript_str_generator(transcript)\n",
    "    prompt = segmentation_prompt_generator(transcript_str)\n",
    "    segmentation_prompts.append(prompt)\n",
    "    interviewees.append(transcript_file.split(\"/\")[-1].split(\".\")[0])\n",
    "    transcripts.append(transcript)\n",
    "responses = gpt.multithread_prompts(openai_client, segmentation_prompts, temperature=0, response_format=\"json\")\n",
    "responses = [json.loads(response)['segments'] for response in responses]\n",
    "for response, interviewee, transcript in zip(responses, interviewees, transcripts):\n",
    "    segmented_transcript = segment_list(transcript, response)\n",
    "    save_json(segmented_transcript, f\"segmented_transcripts/{interviewee}.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyudao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
