{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import gpt as gpt\n",
    "import json\n",
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w', encoding='utf-8') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "\n",
    "# openai\n",
    "openai_api_key = open(\"openai_api_key\").read()\n",
    "openai_client=OpenAI(api_key=openai_api_key)\n",
    "def messages_to_string(messages, pid):\n",
    "    messages_str = \"\"\n",
    "    for index, message in enumerate(messages):\n",
    "        messages_str += str(index) + \". \"\n",
    "        if message['speaker'] == \"1\":\n",
    "            messages_str += \"採訪者: \" + message['content'] + \"\\n\"\n",
    "        elif message['speaker'] == \"0\":\n",
    "            messages_str += pid + \": \" + message['content'] + \"\\n\"\n",
    "    return messages_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def speaker_to_str(speaker, pid):\n",
    "    if speaker == \"1\":\n",
    "        return \"採訪者\"\n",
    "    elif speaker == \"0\":\n",
    "        return pid\n",
    "    else:\n",
    "        return pid\n",
    "def collect_messages(messages, pid):\n",
    "    result = []\n",
    "    for index, message in enumerate(messages):\n",
    "        if message['speaker'] == 1:\n",
    "            previous_message, next_message = \"\", \"\"\n",
    "            if index > 0:\n",
    "                previous_message = speaker_to_str(messages[index-1]['speaker'], pid) + messages[index - 1]['content'] + \"\\n\"\n",
    "            if index < len(messages) - 1:\n",
    "                next_message = speaker_to_str(messages[index+1]['speaker'], pid) + messages[index + 1]['content'] + \"\\n\"\n",
    "            current_message = speaker_to_str(message['speaker'], pid) + message['content'] + \"\\n\"\n",
    "            current_message = \"<target>\" + current_message + \"</target>\\n\"\n",
    "            result.append(previous_message + current_message + next_message)\n",
    "    return result\n",
    "def classify_interviewer_message_prompt(interviewer_messages):\n",
    "    # conversations_str = messages_to_string(interviewer_messages, pid)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"你是一個訪談紀錄的分類系統。現在有一個很長的訪談紀錄，需要你幫忙分類。\n",
    "                用戶的需求是：在訪談紀錄中，有些採訪者的發言是引導新的討論，有些採訪者的發言只是重複之前的討論。\n",
    "                通常，如果發言很長或是有提到具體的名詞，就是引導新的討論.\n",
    "                如果發言很短且只是語助詞，比如‘真的喔？’，‘的確’， ‘有趣’，就不太可能是在引導新的討論.\n",
    "                請根據上下文，判斷被wrap在<target></target>中的採訪者發言是否在引導新的討論，用yes or no來回答。\n",
    "                Please respond with the following JSON format:\n",
    "                {{\n",
    "                    \"answer\": yes or no\n",
    "                }}\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": interviewer_messages\n",
    "        }\n",
    "    ]\n",
    "    return messages\n",
    "def collect_interviewee_messages(transcripts, pid):\n",
    "    # 0: interviewee, 1: interviewer\n",
    "    # speakers = [0 if t['speaker'] == pid else 1 for t in transcripts]\n",
    "    speakers = [t['speaker'] for t in transcripts]\n",
    "    # print(\"speakers:\", speakers)\n",
    "    qa_pairs = []\n",
    "    i = 0\n",
    "    cur_q = []\n",
    "    while i < len(speakers):\n",
    "        if i == 0:\n",
    "            cur_q.append(i)\n",
    "            i += 1\n",
    "            continue\n",
    "        if speakers[i] == 1: # interviewer\n",
    "            qa_pairs.append(cur_q)\n",
    "            cur_q = [i]\n",
    "            i += 1\n",
    "        else: # interviewee\n",
    "            cur_q.append(i)\n",
    "            i += 1\n",
    "    qa_pairs.append(cur_q)\n",
    "    return qa_pairs\n",
    "def qa_index_to_message(qa_pairs, transcripts):\n",
    "    res = []\n",
    "    for qa in qa_pairs:\n",
    "        res.append([transcripts[i]['content'] for i in qa])\n",
    "    return res\n",
    "def classify_interviewer_message(question):\n",
    "    messages = [\n",
    "        # {\n",
    "        #     'role': 'system',\n",
    "        #     'content': \"\"\"\n",
    "        #         The user is dealing with a transcript of an interview.\n",
    "        #         In the transcript, some interviewer messages are just repeating what the interviewee said, these messages are less interesting.\n",
    "        #         Others are for invoking a new discussion, or asking about a new topic in the interview.\n",
    "        #         You are a classification system that helps user decide if a sentence is invoking a new topic.\n",
    "        #         Normally, if the sentence is long it should be yes.\n",
    "        #         Reply with \"yes\" if the sentence is worth further examination, otherwise reply with \"no\".\n",
    "        #     \"\"\"\n",
    "        # },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': \"\"\"\n",
    "                你是一個訪談紀錄的分類系統。現在有一個很長的訪談紀錄，需要你幫忙分類。\n",
    "                用戶的需求是：在訪談紀錄中，有些發言是引導新的討論，有些發言只是重複之前的討論。\n",
    "                通常，如果發言很長或是有提到具體的名詞，就是引導新的討論, 請回答「是」。\n",
    "                如果發言很短且只是語助詞，比如‘真的喔？’，‘的確’， ‘有趣’，就不太可能是在引導新的討論，請回答「否」。\n",
    "                請判斷一個發言是否在引導新的討論，用「是」或「否」來回答。\n",
    "            \"\"\"\n",
    "        },\n",
    "        # example 1\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'name': 'example_user',\n",
    "            'content': '問：這麼多喔。'\n",
    "\n",
    "        },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'name': 'example_system',\n",
    "            'content': '否'\n",
    "        },\n",
    "        # example 2\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'name': 'example_user',\n",
    "            'content': \"\"\"\n",
    "            問：的確，我們這邊就是社會，目前也還沒有說真的要針對哪一塊，但是我們是想要聊解現在這個狀況，就是綠島人民來說就是居民來說不就新居民就居民，他們現在重視有什麼樣的議題。\\n\n",
    "            \"\"\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'name': 'example_system',\n",
    "            'content': '是'\n",
    "        },\n",
    "        # example 3\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'name': 'example_user',\n",
    "            'content': \"\"\"\n",
    "            問：那您就是您覺得您在就是綠島工作，有沒有因為是綠島的緣故，就是讓您工作比較有挑戰性的地方？或者是有，就是特別特別喜歡的地方?\\n\n",
    "            \"\"\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'name': 'example_system',\n",
    "            'content': '是'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': \"問：{}\".format(question)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return gpt.request_chatgpt_gpt4(openai_client, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pprint import pprint\n",
    "# chunk_prompts = []\n",
    "for transcript in glob.glob(\"raw/*.json\"):\n",
    "    print(transcript)\n",
    "    pid = transcript.replace(\".json\", \"\").replace(\"raw/\", \"\")\n",
    "    transcript = json.load(open(transcript))\n",
    "    # print(transcript)\n",
    "    res = []\n",
    "    print(pid)\n",
    "    qa_pairs = collect_interviewee_messages(transcript, pid)\n",
    "    qa_pairs = qa_index_to_message(qa_pairs, transcript) \n",
    "    print(qa_pairs)\n",
    "    for qa_index, qa_pair in enumerate(qa_pairs):\n",
    "        if len(qa_pair) == 1:\n",
    "            qa_pair = ['（訪談開始）', qa_pair[0]]\n",
    "            qa_pairs[qa_index] = qa_pair\n",
    "        if len(qa_pair[0]) < 50 and qa_index > 0 and len(qa_pairs[qa_index-1][1]) < 30:\n",
    "            label = '否'\n",
    "        else:\n",
    "            label = classify_interviewer_message(qa_pair[0])\n",
    "            if label not in ['是', '否']:\n",
    "                label = classify_interviewer_message(qa_pair[0])\n",
    "                if label not in ['是', '否']:\n",
    "                    label = '否'\n",
    "        print(label, len(qa_pair[0]), len(qa_pairs[max(0, qa_index-1)][1]),qa_pair)\n",
    "        res.append({\n",
    "            'sentence': qa_pair[0],\n",
    "            'label': label\n",
    "        })\n",
    "    save_json(res, 'clf/{}.json'.format(pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_topic(qa_messages, speakers, interviewer_clf):\n",
    "    index = 0\n",
    "    interviewer_index = 0\n",
    "    chunks = []\n",
    "    new_chunk = []\n",
    "    for qa_index, qa in enumerate(qa_messages):\n",
    "        for message in qa:\n",
    "            if speakers[index] == 1:\n",
    "                new_chunk.append({\n",
    "                    \"speaker\": 1,\n",
    "                    \"content\": message\n",
    "                })\n",
    "                interviewer_index += 1\n",
    "            else:\n",
    "                new_chunk.append({\n",
    "                    \"speaker\": 0,\n",
    "                    \"content\": message\n",
    "                })\n",
    "            if qa_index < len(qa_messages)-1 and speakers[index+1] == 1 and interviewer_clf[interviewer_index] == 1:\n",
    "                chunks.append(new_chunk)\n",
    "                new_chunk = []\n",
    "            index += 1\n",
    "    chunks.append(new_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcript in glob.glob(\"raw/*.json\"):\n",
    "    print(transcript)\n",
    "    pid = transcript.replace(\".json\", \"\").replace(\"raw/\", \"\")\n",
    "    transcript = json.load(open(transcript))\n",
    "    speakers = [t['speaker'] for t in transcript]\n",
    "    intvwer_clf = json.load(open('clf/{}.json'.format(pid)))\n",
    "    intvwer_clf = [1 if i['label'] == '是' else 0 for i in intvwer_clf]\n",
    "\n",
    "    qa_pairs = collect_interviewee_messages(transcript, pid)\n",
    "    qa_pairs = qa_index_to_message(qa_pairs, transcript) \n",
    "    chunks = chunk_by_topic(qa_pairs, speakers, intvwer_clf) \n",
    "    save_json(chunks, 'chunks/{}.json'.format(pid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat\n",
    "for chunk_file in glob.glob(\"chunks/*.json\"):\n",
    "    participant_id = chunk_file.replace(\".json\", \"\").replace(\"chunks/\", \"\")\n",
    "    chunks = json.load(open(chunk_file))\n",
    "    reformat_chunks = []\n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        reformat_chunks.append({\n",
    "            \"id\": \"{}_{}\".format(participant_id, chunk_index),\n",
    "            \"conversation\": chunk,\n",
    "        })\n",
    "    save_json(reformat_chunks, 'chunk_summaries/{}.json'.format(participant_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toString(chunk):\n",
    "    result = ''\n",
    "    for dialogue in chunk:\n",
    "        if dialogue['speaker'] == '1':\n",
    "            result += \"Interviewer: \"\n",
    "        else:\n",
    "            result += \"Interviewee: \"\n",
    "        # result += dialogue['speaker'] + \": \" + dialogue['content'] + '\\n'\n",
    "        result += dialogue['content'] + \"\\n\"\n",
    "    return result\n",
    "\n",
    "def extract_keywords(paragraph):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a keyword extraction system that extracts keywords from a monologue. \n",
    "            The monologue is about a person who is talking about their life.\n",
    "            The keywords should be the most important words in the monologue.\n",
    "            Use the exact words that the person uses in the monologue.\n",
    "            Replay with a list of keywords in Traditional Chinese in the following JSON format:\n",
    "            {{\n",
    "                \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", ...]\n",
    "            }}\n",
    "            \"\"\"\n",
    "       },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": paragraph\n",
    "       }\n",
    "    ]\n",
    "    # keywords = request_chatgpt_gpt4(messages)\n",
    "    return messages\n",
    "def extract_title(conversation):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a title analysis system. \n",
    "            You are given a conversation between two people: Interviewer and Interviewee. \n",
    "            Give a concise title for the conversation with no more than 10 words.\n",
    "            Reply with Traditional Chinese.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": conversation\n",
    "        }\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def topic_analysis(text, topics):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a topic analysis system. \n",
    "            You are given a conversation between two people: Interviewer and Interviewee. \n",
    "            The conversation is mostly about the environment, but the user cares more about a specific aspect.\n",
    "            What is the focus aspect of the conversation?\n",
    "            Reply with exactly only one of the following choices: {}\n",
    "            Be aware that the user wants to distinguish between water/ocean environment and land environment.\n",
    "            Reply with the following JSON format in Traditional Chinese:\n",
    "            {{ \"result\": string (from one of the provided choices) }}\n",
    "        \"\"\".format(\", \".join(topics))\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }\n",
    "    ]\n",
    "    return messages\n",
    "def emotion_analysis(text, emotions):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a emotion analysis system. \n",
    "            You are given a conversation between two people: Interviewer and Interviewee. \n",
    "            What is the overall emotion of the Interviewee?\n",
    "            Reply with exactly only one of the following emotions: {}\n",
    "            Reply with the following JSON format:\n",
    "            {{ \"emotion\": string (from one of the provided choices) }}\n",
    "        \"\"\".format(\", \".join(emotions))\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Conversation: \" + text\n",
    "        }\n",
    "    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "topics = ['農業','商業', '觀光', '交通','整體經濟','能源','災害','貿易','政府運作','住屋','醫療','公有土地', '其他', '垃圾處理' , '汙染' , '海域生態環境', '陸地生態環境']\n",
    "emotions = [\"proud\", \"resigned\", \"angry\", \"worried\", \"neutral\"]\n",
    "for interview_file in glob.glob(\"chunk_summaries/*.json\"):\n",
    "    pid = interview_file.replace(\".json\", \"\").replace(\"chunk_summaries/\", \"\")\n",
    "    interview_data = json.load(open(interview_file))\n",
    "    print(interview_file)\n",
    "    keyword_prompts = []\n",
    "    title_prompts = []\n",
    "    topic_prompts = []\n",
    "    emotion_prompts = []\n",
    "    for chunk in interview_data:\n",
    "        interviewee_messages = \"\\n\".join([message['content'] for message in chunk['conversation'] if message['speaker'] == \"0\"])\n",
    "        chunk_messages = toString(chunk['conversation'])\n",
    "        keyword_prompts.append(extract_keywords(interviewee_messages))\n",
    "        title_prompts.append(extract_title(chunk_messages))\n",
    "        topic_prompts.append(topic_analysis(chunk_messages, topics))\n",
    "        emotion_prompts.append(emotion_analysis(chunk_messages, emotions))\n",
    "\n",
    "    chunk_keywords = gpt.multithread_prompts(openai_client, keyword_prompts)\n",
    "    chunk_titles = gpt.multithread_prompts(openai_client, title_prompts)\n",
    "    chunk_topics = gpt.multithread_prompts(openai_client, topic_prompts, format=\"json\")\n",
    "    chunk_emotions = gpt.multithread_prompts(openai_client, emotion_prompts, format=\"json\")\n",
    "    result = []\n",
    "    for chunk, keywords, title, topic, emotion in zip(interview_data, chunk_keywords, chunk_titles, chunk_topics, chunk_emotions):\n",
    "        chunk_messages = toString(chunk['conversation'])\n",
    "        chunk['raw_keywords'] = keywords\n",
    "        chunk['title'] = title\n",
    "        # topic\n",
    "        topic_max_retry = 3\n",
    "        try:\n",
    "            topic = json.loads(topic)['result']\n",
    "        except:\n",
    "            topic = \"error\"\n",
    "        chunk['topic'] = topic\n",
    "        # emotion\n",
    "        emotion_max_retry = 3\n",
    "        try:\n",
    "            emotion = json.loads(emotion)['emotion']\n",
    "        except:\n",
    "            emotion = \"error\"\n",
    "        chunk['emotion'] = emotion\n",
    "        result.append(chunk)\n",
    "    save_json(result, \"chunk_summaries_w_ktte/{}.json\".format(pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "topics = ['農業','商業','文化', '觀光', '交通','整體經濟','能源','災害','貿易','政府運作','住屋','醫療','公有土地', '其他', '垃圾處理' , '汙染' , '海域生態環境', '陸地生態環境']\n",
    "emotions = [\"proud\", \"resigned\", \"angry\", \"worried\", \"neutral\"]\n",
    "index = 0\n",
    "for interview_file in glob.glob(\"chunk_summaries_w_ktte/*.json\"):\n",
    "    # if index == 0: \n",
    "    #     index += 1\n",
    "    #     continue\n",
    "    pid = interview_file.replace(\".json\", \"\").replace(\"chunk_summaries_w_ktte/\", \"\")\n",
    "    chunks = json.load(open(interview_file))\n",
    "    # topic\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        if chunk['topic'] in topics: continue\n",
    "        retry = 5\n",
    "        topic = chunk['topic']\n",
    "        while topic == \"error\" or topic not in topics:\n",
    "            topic = gpt.request_chatgpt_gpt4(openai_client, topic_analysis(toString(chunk['conversation']), topics))\n",
    "            topic = topic.replace(\"```\", \"\").replace(\"json\", \"\")\n",
    "            print(\"retrying topic...\", topic)\n",
    "            topic = json.loads(topic)['result']\n",
    "            if topic == \"土地生態環境\": topic = \"陸地生態環境\"\n",
    "            # print(topic, topic in topics)\n",
    "            retry -= 1\n",
    "            if retry == 0:\n",
    "                topic = \"其他\"\n",
    "                print(\"retry out.................\")\n",
    "                break\n",
    "        chunks[index]['topic'] = topic\n",
    "    # emotion\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        if chunk['emotion'] in emotions: continue\n",
    "        retry = 3\n",
    "        while chunk['emotion'] == \"error\" or chunk['emotion'] not in emotions:\n",
    "            emotion = gpt.request_chatgpt_gpt4(openai_client, emotion_analysis(toString(chunk['conversation']), emotions))\n",
    "            print(\"retrying emotion...\", emotion)\n",
    "            emotion = json.loads(emotion)['emotion']\n",
    "            retry -= 1\n",
    "            if retry == 0:\n",
    "                break\n",
    "        chunks[index]['emotion'] = emotion\n",
    "    save_json(chunks, \"chunk_summaries_w_ktte/{}.json\".format(pid))\n",
    "    # break\n",
    "# while topic not in topics and topic_max_retry > 0:\n",
    "#     topic = json.loads(gpt.request_chatgpt_gpt4(openai_client, topic_analysis(chunk_messages, topics)))['result']\n",
    "#     topic_max_retry -= 1\n",
    "#     print(\"retrying topic...\", topic)\n",
    "# emotion = json.loads(emotion)['emotion']\n",
    "# while emotion not in emotions and emotion_max_retry > 0:\n",
    "#     emotion = gpt.request_chatgpt_gpt4(openai_client, emotion_analysis(chunk_messages, emotions))\n",
    "#     emotion_max_retry -= 1\n",
    "#     print(\"retrying emotion...\", emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interview_file in glob.glob(\"chunk_summaries_w_ktte/*.json\"):\n",
    "    pid = interview_file.replace(\".json\", \"\").replace(\"chunk_summaries_w_ktte/\", \"\")\n",
    "    chunks = json.load(open(interview_file))\n",
    "    # topic\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        if chunk['topic'] not in topics: print(chunk['topic'])\n",
    "    # emotion\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        if chunk['emotion'] not in emotions: print(chunk['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for interview_file in glob.glob(\"chunk_summaries/*.json\"):\n",
    "    pid = interview_file.replace(\".json\", \"\").replace(\"chunk_summaries/\", \"\")\n",
    "    interview_data = json.load(open(interview_file))\n",
    "    chunk_contents = [toString(chunk['conversation']) for chunk in interview_data]\n",
    "    chunk_embeddings = gpt.multithread_embeddings(openai_client, chunk_contents)\n",
    "    for chunk, embedding in zip(interview_data, chunk_embeddings):\n",
    "        chunk['embedding'] = embedding\n",
    "    save_json(interview_data, \"chunk_embeddings/{}.json\".format(pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_prompt(keyword):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a dictionary system.\n",
    "            The user wants to know the meaning of a specific word in the context of environment, ecology, an related topics.\n",
    "            Reply in Traditional Chinese with the definition of the word.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"請解釋一下「{}」\".format(keyword)\n",
    "        }\n",
    "    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_file = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interview_file in glob.glob(\"chunk_summaries_w_ktte/*.json\"):\n",
    "    print(interview_file)\n",
    "    interview_data = json.load(open(interview_file))\n",
    "    keyword_prompts = []\n",
    "    for chunk in interview_data:\n",
    "        interviewee_messages = \"\\n\".join([message['content'] for message in chunk['conversation'] if str(message['speaker']) == \"0\"])\n",
    "        keyword_prompts.append(extract_keywords(interviewee_messages))\n",
    "    keywords_by_chunk = gpt.multithread_prompts(openai_client, keyword_prompts, format=\"json\")\n",
    "    index = 0\n",
    "    for chunk, keywords in zip(interview_data, keywords_by_chunk):\n",
    "        keywords = json.loads(keywords)['keywords']\n",
    "        print(keywords)\n",
    "        interview_data[index]['raw_keywords'] = keywords\n",
    "        index += 1\n",
    "    save_json(interview_data, interview_file)\n",
    "    done_file.append(interview_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_summaries_w_ktte/N6.json\n",
      "['生活', '綠島人', '生態']\n",
      "['家', '環境', '生活', '擁有', '地方', '收入', '生活價值', '感受', '海', '重要的人', '歸屬感']\n",
      "['環境', '人']\n",
      "['綠島', '潛水', '海洋資源', '打工換宿', '乾淨', '適合居住', '蘭嶼', '硬珊瑚礁', '珊瑚礁', '魚類', '當地文化', '接受度']\n",
      "['蘭嶼', '原住民', '新住民', '排外', '朋友', '開店', '章魚乾']\n",
      "['傳統文化', '排外', '移來', '土生土長', '歲數', '民宿業', '競爭', '原住民', '外地人']\n",
      "['排外', '潛水店', '新的', '舊的', '流言緋語', '生意']\n",
      "['生活', '感覺', '留言', '蜚語']\n",
      "['挑戰', '獨立生活', '潛水', '縣市']\n",
      "['觀光旅遊業', '船班問題', '天氣', '風大', '雨大', '浪大', '客人', '舒服', '船班班次', '不順利']\n",
      "['船', '航權', '班次', '困擾', '取消', '客人', '規則']\n",
      "['延後', '原因', '浪高', '停船', '風平浪靜', '說的算', '吵架', '船班', '公司', '取消']\n",
      "['船班', '公司', '家族企業', '權利']\n",
      "['漁會', '船公司', '碼頭', '漁獲', '協議', '臺東', '爭吵', '撥款', '居民', '生活經濟']\n",
      "['地人', '連署', '簽名', '抗議', '綠島', '資訊']\n",
      "['醫療衛生', '交通', '政府運作', '生態', '總體經濟']\n",
      "['醫療', '長輩', '臺東', '資源', '車禍', '直升機', '搶救']\n",
      "['外國人', '騎車', '不熟路', '機車', '交通', '船舶', '飛機班次', '小琉球', '公船', '規定時間', '船公司', '票價', '調整', '班次增加', '小型飛機', '舒服', '浪', '15分鐘', '20分鐘', '選擇']\n",
      "['船的班次', '公船', '飛機', '來回', '半小時', '加開']\n",
      "['班次', '票價', '飛機', '綠島', '觀光', '政府', '民宿', '活動', '演唱會', '人權', '臺東', '臺東縣', '長濱', '池上']\n",
      "['開罰', '政府', '拆遷隊', '繳費用', '防風林', '拆除', '鐵皮屋', '中寮', '奇巖路', '柴口', '防楓林', '蓋']\n",
      "['基本上', '邊坡保護區', '海邊', '檢舉', '政府', '拆遷', '罰款']\n",
      "['人生', '重要', '回憶', '成長', '挑戰', '夢想', '家人', '朋友', '愛情', '自我']\n",
      "['費用', '拆遷', '吊車', '撥款', '爭議', '當地人', '政府']\n",
      "['鐵皮屋', '檢舉', '颱風', '貨櫃屋', '根生比較久的人', '外地人', '可怕', '亂蓋', '建物', '勢力', '拆']\n",
      "['莫德納', '風', '水泥房子', '水塔', '木屋', '大馬路', '床', '危險', '恐怖']\n",
      "['颱風', '最大', '浪', '雨', '莫德納', '中央政府', '蔡英文', '總統', '民宅', '受損', '房子']\n",
      "['影響', '偶然', '夏天', '冬天', '東北季風', '颱風', '日本', '機率']\n",
      "['政府', '管理', '責任', '問題', '解決']\n",
      "['環境生態', '海洋資源', '珊瑚礁', '珊瑚白化', '海水溫度', '產卵', '觀音生日']\n",
      "['冬天', '水溫', '25度', '陸地', '熱']\n",
      "['排水問題', '污水處理廠', '海域', '生態死亡', '螢火蟲', '濕度', '防風林']\n",
      "['污水處理廠', '環境問題', '垃圾問題', '垃圾場', '火災', '環境污染', '環境生態']\n",
      "['污水處理廠', '提案', '防風林', '排水', '程序', '機器', '髒污', '汙水排水場']\n",
      "['堆著', '久', '長一段時間', '垃圾', '運出去', '擠在那裡']\n",
      "['焚化爐', '人數', '地方', '地址', '破壞', '臺東', '小島']\n",
      "['人比較少', '長期雇用', '管理', '天氣影響', '有利益']\n",
      "['環境', '環保', '資源', '海洋資源', '觀光', '產業', '島嶼', '環保餐具', '環保筷', '環保局', '環保杯', '冬天', '人力不足']\n",
      "['環保杯', '環保署', '冬天', '推行', '政府', '觀光', '總體經濟', '東縣政府', '經費', '臺東', '工作室', '技能班', '觀光產業', '漁民', '漁港', '海鮮', '魚市場', '沖繩']\n",
      "['芡粿', '章魚乾', '飛魚', '花生豆腐', '政府', '經費', '博物館', '咕咾石', '綠島', '歷史', '人權', '野百合', '女性', '自力更生', '推廣', '出口']\n",
      "['生活', '家業', '工作', '年輕', '小孩', '結婚', '資源', '老婆', '外面', '沒有']\n",
      "['男性', '強悍', '享受', '打魚', '特別', '縣市', '申請槍枝', '非保育區', '章魚', '壓', '沖繩', '海女文化']\n",
      "['鉤子', '自給自足', '文化', '打魚', '特別', '模式', '文化存在', '意義', '保留', '生存', '程序', '觀光']\n",
      "['享受', '抽煙', '喝酒', '打麻將', '女性', '縫縫線', '男生', '耕種', '花', '研究院', '花圃', '山壁', '燕子洞口', '白色野百合']\n",
      "['能源', '環保', '生態', '破壞', '姿態', '電', '水', '污水處理廠', '焚化爐', '選擇']\n",
      "['星星', '能源', '存在', '需要', '破壞', '捨棄']\n",
      "['平衡', '政府', '環保', '觀光', '生態', '海洋動物', '花蓮', '翻車魚', '捕撈', '保育', '廣告']\n",
      "['觀光', '破壞', '環境', '推動']\n",
      "['配套措施', '觀光', '沐浴乳', '環保', '民宿', '洗髮精', '海水', '污染', '改善', '機率']\n",
      "['排名最低', '想了解', '住屋', '公有土地']\n",
      "['公有土地', '總體經濟', '協助婦女', '就業', '政府', '租金', '年輕人', '創業', '觀光產業', '國內貿易']\n",
      "['管道', '民宿業', '浮潛', '體力活', '阿姨', '煮', '編織', '國內貿易', '物資', '貿易部分', '觀光產業', '收入']\n",
      "['政府', '運作']\n",
      "['認真', '想一想', '回答', '遺漏', '真的']\n",
      "['美好的未來', '綠島', '想像', '海洋', '環境', '期許', '保持', '狀態', '潛水', '市場', '連結']\n",
      "['研究', '部分']\n",
      "['海洋', '保育', '生態', '意識', '海龜', '玩樂', '環境', '研究']\n",
      "['照片', '晚霞', '燈塔', '海', '美', '火燒島', '火燒雲', '橘黃色', '燃燒', '堅強', '強壯']\n",
      "['溫泉', '觀光', '營業', '日出', '溫泉設施', '私營', '政府自營', '免費', '觀光味道', '礁溪', '園區', '泡腳', '私營', '淘汰']\n",
      "['破壞', '生態', '海底溫泉', '建設', '非自然', '營運', '觀光客', '私人營利', '朝日美名', '開玩笑']\n",
      "['當地人', '溫泉', '朝日溫泉']\n",
      "['經營', '營運', '設施', '公營', '私營', '停業', '人權', '流馬溝', '政治犯', '象鼻岩']\n",
      "['燈塔', '管理員', '工作成效', '管理程序', '空', '進去', '門', '營業', '飛機起飛', '綠島']\n",
      "['公園', '小長城', '硬的地方', '不喜歡']\n",
      "['硬', '寵物1', '寵物2']\n",
      "['換宿', '陰的狀態', '戶外面', '護欄', '漂亮', '夜深', '哈巴狗']\n",
      "['好人', '親切', '長輩', '年紀', '資歷', '遇到']\n",
      "['年輕人', '選擇', '因素']\n",
      "['抗拒', '接受度', '慢熟型', '慢熱型', '推動', '環保杯', '客人', '使用', '推', '買飲料', '看到', '做', '被動', '主動式']\n",
      "['魚類', '爭議', '敏感', '漁船', '漁獲', '反彈', '保育區', '資源']\n",
      "['捕魚', '維生', '娛樂性質', '當地人', '管制', '下海', '自己吃']\n",
      "['自主', '鄰居', '外地人', '賣', '吃']\n",
      "['漁船', '捕撈', '放流刺網', '海邊', '勾到', '死亡']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "for interview_file in glob.glob(\"chunk_summaries_w_ktte/*.json\"):\n",
    "    print(interview_file)\n",
    "    interview_data = json.load(open(interview_file))\n",
    "    for chunk in interview_data:\n",
    "        print(chunk['raw_keywords'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "keyword_freq = defaultdict(int)\n",
    "for interview_file in glob.glob(\"chunk_summaries_w_ktte/*.json\"):\n",
    "    print(interview_file)\n",
    "    interview_data = json.load(open(interview_file))\n",
    "    for chunk in interview_data:\n",
    "        keywords = chunk['raw_keywords']\n",
    "        for k in keywords:\n",
    "            keyword_freq[k] += 1\n",
    "\n",
    "# filter out keywords that only appear once\n",
    "all_keywords = [k for k in keyword_freq.keys() if keyword_freq[k] > 1]\n",
    "keyword_statistics = [{\"keyword\": k, \"frequency\": keyword_freq[k]} for k in all_keywords]\n",
    "explain_prompts = [explain_prompt(keyword) for keyword in all_keywords]\n",
    "explanations = gpt.multithread_prompts(openai_client, explain_prompts)\n",
    "embeddings = gpt.multithread_embeddings(openai_client, explanations)\n",
    "result = []\n",
    "for keyword, explanation, embedding in zip(all_keywords, explanations, embeddings):\n",
    "    result.append({\n",
    "        \"keyword\": keyword,\n",
    "        \"explanation\": explanation,\n",
    "        \"embeddings\": embeddings\n",
    "    })\n",
    "save_json(result, \"result/keywords.json\")\n",
    "save_json(keyword_statistics, \"result/keyword_statistics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for keyword, explanation, embedding in zip(all_keywords, explanations, embeddings):\n",
    "    result.append({\n",
    "        \"keyword\": keyword,\n",
    "        \"explanation\": explanation,\n",
    "        \"embedding\": embedding\n",
    "    })\n",
    "save_json(result, \"result/keywords.json\")\n",
    "save_json(keyword_statistics, \"result/keyword_statistics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyudao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
