{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from pprint import pprint\n",
    "import requests\n",
    "api_key = open(\"api_key\").read()\n",
    "openai.api_key = api_key\n",
    "\n",
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w', encoding='utf-8') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "\n",
    "def request_chatgpt_gpt4(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k-0613\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   # text = text.replace(\"\\n\", \" \")\n",
    "   # return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "   url = 'https://api.openai.com/v1/embeddings'\n",
    "   headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'Authorization': \"Bearer {}\".format(api_key)\n",
    "   }\n",
    "   data = {\n",
    "      \"input\": text,\n",
    "      \"model\": model\n",
    "   }\n",
    "   res = requests.post(url, headers=headers, json=data)\n",
    "   res = res.json()\n",
    "   return res['data'][0]['embedding']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying the traditional way: tokenize -> stop word removal -> word count\n",
    "# conclusion: does not work well\n",
    "import jieba\n",
    "import chinese_converter\n",
    "from collections import defaultdict\n",
    "# read stopwords from stopwords-master/all.txt\n",
    "stopwords = open('stopwords-master/all.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "stopwords+= ['说', '做', '讲', '东西', '真的', '事情', '是因为', '这件', '…']\n",
    "def clean(messages):\n",
    "    res = []\n",
    "    for message in messages:\n",
    "        simp_message = chinese_converter.to_simplified(message)\n",
    "        tokens = [token for token in jieba.cut(simp_message, cut_all=False) if token not in stopwords]\n",
    "        res.append(tokens)\n",
    "    return res\n",
    "def freq(tokens_list):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for tokens in tokens_list:\n",
    "        for token in tokens:\n",
    "            freq_dict[token] += 1\n",
    "    freq_dict = {k: v for k, v in sorted(freq_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return freq_dict\n",
    "tokens_list = clean(interviewee_messages)\n",
    "for message in tokens_list:\n",
    "    freq_dict = freq([message])\n",
    "    print(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final script: 1-3\n",
    "# 1. transform transcript to qa pairs\n",
    "def collect_interviewee_messages(transcripts):\n",
    "    # 0: interviewee, 1: interviewer\n",
    "    speakers = [0 if t['speaker'] == participant_id else 1 for t in transcripts]\n",
    "    qa_pairs = []\n",
    "    i = 0\n",
    "    cur_q = []\n",
    "    while i < len(speakers):\n",
    "        if i == 0:\n",
    "            cur_q.append(i)\n",
    "            i += 1\n",
    "            continue\n",
    "        if speakers[i] == 1: # interviewer\n",
    "            qa_pairs.append(cur_q)\n",
    "            cur_q = [i]\n",
    "            i += 1\n",
    "        else: # interviewee\n",
    "            cur_q.append(i)\n",
    "            i += 1\n",
    "    qa_pairs.append(cur_q)\n",
    "    return qa_pairs\n",
    "def qa_index_to_message(qa_pairs, transcripts):\n",
    "    res = []\n",
    "    for qa in qa_pairs:\n",
    "        res.append([transcripts[i]['content'] for i in qa])\n",
    "    return res\n",
    "\n",
    "participant_id = 'N6'\n",
    "for i in range(6, 20):\n",
    "    participant_id = 'N{}'.format(i)\n",
    "    transcripts = json.load(open('../data/raw/transcript/json/{}_done.json'.format(participant_id)))\n",
    "    interviewee_messages = [t['content'] for t in transcripts if t['speaker'] == participant_id]\n",
    "    interviewer_messages = [t['content'] for t in transcripts if t['speaker'] == '採訪者']\n",
    "    qa_pairs = collect_interviewee_messages(transcripts)\n",
    "    qa_messages = qa_index_to_message(qa_pairs, transcripts)\n",
    "    save_json(qa_messages, '../data/result/tmp/qa_messages_{}.json'.format(participant_id))\n",
    "# print(len(qa_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. classify interviewer messages with chatgpt\n",
    "def classify_interviewer_message(interviewer_message):\n",
    "    message = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': \"\"\"\n",
    "                The user is dealing with a transcript of an interview.\n",
    "                In the transcript, some interviewer messages are just repeating what the interviewee said, these messages are less interesting.\n",
    "                Others are for invoking a new discussion, or asking about a new topic in the interview.\n",
    "                You are a classification system that helps user decide if a sentence is invoking a new topic.\n",
    "                Normally, if the sentence is long it should be yes.\n",
    "                Reply with \"yes\" if the sentence is worth further examination, otherwise reply with \"no\".\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': interviewer_message\n",
    "        }\n",
    "    ]\n",
    "    response = request_chatgpt_gpt4(message)\n",
    "    return response\n",
    "\n",
    "for i in range(5, 20):\n",
    "    participant_id = 'N{}'.format(i)\n",
    "    transcripts = json.load(open('../data/raw/transcript/json/{}_done.json'.format(participant_id)))\n",
    "    interviewee_messages = [t['content'] for t in transcripts if t['speaker'] == participant_id]\n",
    "    interviewer_messages = [t['content'] for t in transcripts if t['speaker'] == '採訪者']\n",
    "    res = []\n",
    "    print(\"participant_id: {}\".format(participant_id))\n",
    "    for interviewer_message in interviewer_messages:\n",
    "        label = classify_interviewer_message(interviewer_message)\n",
    "        res.append({\n",
    "            'sentence': interviewer_message,\n",
    "            'label': label\n",
    "        })\n",
    "    save_json(res, '../data/result/tmp/interviewer_message_classification_{}.json'.format(participant_id))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_id: N4\n",
      "participant_id: N5\n",
      "participant_id: N6\n",
      "participant_id: N7\n",
      "participant_id: N8\n",
      "participant_id: N9\n",
      "participant_id: N10\n",
      "participant_id: N11\n",
      "participant_id: N12\n",
      "participant_id: N13\n",
      "participant_id: N14\n",
      "participant_id: N15\n",
      "participant_id: N16\n",
      "participant_id: N17\n",
      "participant_id: N18\n",
      "participant_id: N19\n"
     ]
    }
   ],
   "source": [
    "# 3. chunk qa messages by topic \n",
    "def chunk_by_topic(qa_messages, speakers, interviewer_clf):\n",
    "    index = 0\n",
    "    interviewer_index = 0\n",
    "    chunks = []\n",
    "    new_chunk = []\n",
    "    for qa_index, qa in enumerate(qa_messages):\n",
    "        for message in qa:\n",
    "            if speakers[index] == 1:\n",
    "                new_chunk.append({\n",
    "                    \"speaker\": 1,\n",
    "                    \"content\": message\n",
    "                })\n",
    "                interviewer_index += 1\n",
    "            else:\n",
    "                new_chunk.append({\n",
    "                    \"speaker\": 0,\n",
    "                    \"content\": message\n",
    "                })\n",
    "            if qa_index < len(qa_messages)-1 and speakers[index+1] == 1 and interviewer_clf[interviewer_index] == 1:\n",
    "                chunks.append(new_chunk)\n",
    "                new_chunk = []\n",
    "            index += 1\n",
    "    chunks.append(new_chunk)\n",
    "    return chunks\n",
    "for i in range(4, 20):\n",
    "    participant_id = 'N{}'.format(i)\n",
    "    transcripts = json.load(open('../data/raw/transcript/json/{}_done.json'.format(participant_id)))\n",
    "    res = []\n",
    "    print(\"participant_id: {}\".format(participant_id))\n",
    "    qa_messages = json.load(open('../data/result/tmp/qa_messages_{}.json'.format(participant_id)))\n",
    "    speakers = [0 if t['speaker'] == participant_id else 1 for t in transcripts]\n",
    "    intvwer_clf = json.load(open('../data/result/tmp/interviewer_message_classification_{}.json'.format(participant_id)))\n",
    "    intvwer_clf = [1 if i['label'] == 'yes' else 0 for i in intvwer_clf]\n",
    "    chunks = chunk_by_topic(qa_messages, speakers, intvwer_clf) \n",
    "    save_json(chunks, '../data/result/chunks/chunks_{}.json'.format(participant_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings (dep)\n",
    "embeddings = []\n",
    "for index, message in enumerate(interviewee_messages):\n",
    "    print('{}/{}'.format(index, len(interviewee_messages)))\n",
    "    embedding = get_embedding(message)\n",
    "    embeddings.append({\n",
    "        'content': message,\n",
    "        'embedding': embedding\n",
    "    })\n",
    "save_json(embeddings, '../data/result/interviewee_embeddings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sim matrix (dep)\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embeddings = json.load(open('../data/result/interviewee_embeddings.json'))\n",
    "vecs = [e['embedding'] for e in embeddings]\n",
    "sim_matrix = cosine_similarity(np.array(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_embedding(sim_matrix):\n",
    "    chunks = []\n",
    "    length = sim_matrix.shape[0]\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        prev_sim = 1\n",
    "        for j in range(i+1, length):\n",
    "            if j == i+1:\n",
    "                if sim_matrix[i][j] > 0.85:\n",
    "                    prev_sim = sim_matrix[i][j]\n",
    "                    chunks.append((i, j, prev_sim))\n",
    "                    continue\n",
    "                else:\n",
    "                    chunks.append((i, i, 1))\n",
    "                    i = i+1\n",
    "                    break\n",
    "            if sim_matrix[i][j] < 0.85 or prev_sim - sim_matrix[i][j] > 0.04:\n",
    "                i = j\n",
    "                break\n",
    "            else:\n",
    "                prev_sim = sim_matrix[i][j]\n",
    "                chunks.append((i, j, prev_sim))\n",
    "        if j == length-1:\n",
    "            i = length\n",
    "            break\n",
    "    return chunks\n",
    "chunks = chunk_by_embedding(sim_matrix)\n",
    "pprint(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_chunks(chunks_index, interviewee_messages):\n",
    "    chunks = defaultdict(list)\n",
    "    for i, j, _ in chunks_index:\n",
    "        if chunks[i] == []:\n",
    "            chunks[i].append(interviewee_messages[i])\n",
    "        if i == j: continue\n",
    "        chunks[i].append(interviewee_messages[j])\n",
    "    return list(chunks.values())\n",
    "message_chunks = collect_chunks(chunks, interviewee_messages)\n",
    "pprint(message_chunks[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sim matrix distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(data):\n",
    "    x_values = list(range(1, len(data) + 1))\n",
    "\n",
    "    # Create a line chart\n",
    "    plt.plot(x_values, data, marker='o', linestyle='-')\n",
    "    plt.title('Line Chart of similarity')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    # Display the chart (if you're using a Jupyter Notebook, you can omit this line)\n",
    "    plt.show()\n",
    "def exclude(data, index):\n",
    "    return list(data)[:index] + list(data)[index+1:]\n",
    "plot(exclude(sim_matrix[2], 2))\n",
    "plot(exclude(sim_matrix[3], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_chunks = json.load(open('../data/result/chunks_N1.json'))\n",
    "print(len(interviewee_messages), len(response_chunks))\n",
    "summaries = []\n",
    "for chunk in response_chunks:\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': \"\"\"\n",
    "                You are a reporter in Taiwan. \n",
    "                You interviewed a person who is a local residence in Lyudao (綠島), a small island near Taiwan. \n",
    "                Please summarize the interview. Reply in traditional Chinese.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Interview: \\n' + \"\\n\".join(chunk)\n",
    "        }\n",
    "    ]\n",
    "    res = request_chatgpt_gpt4(messages)\n",
    "    summaries.append(res)\n",
    "    print(res)\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(summaries, '../data/result/chunk_summaries.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyudao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
