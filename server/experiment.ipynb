{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from GPTUtils import query, prompts\n",
    "from openai import OpenAI\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "# openai (temp)\n",
    "openai_api_key = open(\"openai_api_key\").read()\n",
    "openai_client=OpenAI(api_key=openai_api_key, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables):\n",
    "#     identify_var_type_prompt_list = []\n",
    "#     response_format, extract_response_func = None, None\n",
    "#     for chunk in all_chunks:\n",
    "#         conversation = chunk['conversation']\n",
    "#         prompt_variables['conversation'] = conversation_to_string(conversation)\n",
    "#         identify_var_type_prompt, response_format, extract_response_func = prompts.identify_var_type_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
    "#         identify_var_type_prompt_list.append(identify_var_type_prompt)\n",
    "#     identified_var_types = query.multithread_prompts(openai_client, identify_var_type_prompt_list, response_format=response_format, temperature=0.0)\n",
    "#     if response_format == 'json':\n",
    "#         identified_var_types = [extract_response_func(i) for i in identified_var_types]\n",
    "#     for (chunk_index, var_types) in enumerate(identified_var_types):\n",
    "#         chunk = all_chunks[chunk_index]\n",
    "#         chunk['var_types'] = var_types\n",
    "#     return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_type_definitions = {\n",
    "#     \"driver\": \"fundamental human causes that lead to certain impacts on the environment to meet basic human needs.\",\n",
    "#     \"pressure\": \"negative phenomena or activities affecting the environment or ecosystems, which are caused by drivers or occur naturally.\",\n",
    "#     \"state\": \"the quantity and quality of physical, chemical, and biological phenomena within a specific timeframe and area.\",\n",
    "#     \"impact\": \"adverse changes in environmental conditions, ecosystem functions, or human well-being.\",\n",
    "#     \"response\": \"any behavior, action, or effort to protect the environment, address environmental issues, or be environmentally friendly.\"\n",
    "# }\n",
    "# for variable_definition_file in glob.glob(\"contexts/variable_definitions/*.json\"):\n",
    "#     variable_definitions = json.load(open(variable_definition_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_variables = {\n",
    "#     \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
    "# }\n",
    "# prompts = json.load(open('prompts/identify_var_types.json'))\n",
    "# system_prompt_blocks = prompts['system_prompt_blocks']\n",
    "# user_prompt_blocks = prompts['user_prompt_blocks']\n",
    "# all_chunks = []\n",
    "# for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
    "#     chunks = json.load(open(chunk_file))\n",
    "#     all_chunks += chunks\n",
    "# # identify the variables in three prompts:\n",
    "# # First, identify if there are any mentions about the var types by extracting the sentences that mention the var types\n",
    "# # all_chunks = all_chunks[:1] # test run\n",
    "# system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
    "# user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
    "# all_chunks = identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
    "\n",
    "# If so, identify the corresponding variables by assigning variables to the sentences\n",
    "# If no existing variables can be matched, assign it to 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/v2/tmp/pipeline/init/chunks.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     all_chunks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunks\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mlen\u001b[39m(all_chunks)\n\u001b[0;32m----> 6\u001b[0m save_json(all_chunks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/v2/tmp/pipeline/init/chunks.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36msave_json\u001b[0;34m(data, filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_json\u001b[39m(data, filename):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/lyudao/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/v2/tmp/pipeline/init/chunks.json'"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
    "    chunks = json.load(open(chunk_file))\n",
    "    all_chunks += chunks\n",
    "len(all_chunks)\n",
    "save_json(all_chunks, \"data/v2/tmp/pipeline/init/chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598/598 [00:20<00:00, 28.84it/s]\n"
     ]
    }
   ],
   "source": [
    "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
    "prompts = json.load(open('GPTUtils/prompts/identify_var_types.json'))\n",
    "system_prompt_blocks = prompts['system_prompt_blocks']\n",
    "user_prompt_blocks = prompts['user_prompt_blocks']\n",
    "prompt_variables = {\n",
    "    \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
    "}\n",
    "all_chunks = json.load(open(\"data/v2/tmp/pipeline/init/chunks.json\"))\n",
    "print(len(all_chunks))\n",
    "# all_chunks = all_chunks[:10]\n",
    "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
    "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
    "all_chunks = query.identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
    "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    var_type_result = chunk['identify_var_types_result']\n",
    "    var_type_result = list(filter(lambda x: x[\"var_type\"] != 'none', var_type_result))\n",
    "    chunk['identify_var_types_result'] = var_type_result\n",
    "\n",
    "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types_filtered.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 774/774 [00:36<00:00, 20.99it/s]\n"
     ]
    }
   ],
   "source": [
    "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
    "var_definitions = {}\n",
    "for var_type in var_type_definitions.keys():\n",
    "    var_definitions_by_type = json.load(open(f\"GPTUtils/contexts/variable_definitions/{var_type}_variables_def.json\"))\n",
    "    var_definitions[var_type] = var_definitions_by_type\n",
    "\n",
    "prompts = json.load(open('GPTUtils/prompts/identify_vars.json'))\n",
    "system_prompt_blocks = prompts['system_prompt_blocks']\n",
    "user_prompt_blocks = prompts['user_prompt_blocks']\n",
    "prompt_variables = {}\n",
    "for var_type, var_type_def in var_type_definitions.items():\n",
    "    prompt_variables[var_type] = {\n",
    "        \"definition\": var_type_def,\n",
    "        \"vars\": \"\\n\".join([f\"{var_name}: {var_def}\" for var_name, var_def in var_definitions[var_type].items()])\n",
    "    }\n",
    "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
    "# all_chunks = all_chunks[:10]\n",
    "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
    "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
    "all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
    "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_vars/chunk_w_vars.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: tokens:  322 False\n",
      "tokens:  255 False\n",
      " 96 False\n",
      "tokens:  468 False\n",
      "tokens:  128 False\n",
      "tokens:  404 False\n",
      "tokens:  431 False\n",
      "tokens:  284 False\n",
      "tokens:  84 False\n",
      "tokens:  267 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.21it/s]\n"
     ]
    }
   ],
   "source": [
    "all_chunks = json.load(open(\"data/v2/user/pipeline/init/chunks.json\"))\n",
    "all_chunks = all_chunks[:10]\n",
    "chunk_conversations = [query.conversation_to_string(chunk['conversation']) for chunk in all_chunks]\n",
    "chunk_embeddings = query.multithread_embeddings(openai_client, chunk_conversations)\n",
    "res = []\n",
    "for (chunk_index, chunk_embedding) in enumerate(chunk_embeddings):\n",
    "    res.append({\n",
    "        \"id\": all_chunks[chunk_index]['id'],\n",
    "        \"embedding\": chunk_embedding\n",
    "    })\n",
    "save_json(res, \"data/v2/user/pipeline/init/chunk_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_id': 'N6_3',\n",
       " 'var1': '旅遊業',\n",
       " 'var2': '人口',\n",
       " 'indicator1': 'driver',\n",
       " 'indicator2': 'driver',\n",
       " 'response': {'relationship': '', 'evidence': ''}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "def process_links(chunk_w_vars):\n",
    "    links = []\n",
    "    for chunk in chunk_w_vars:\n",
    "        chunk_id = chunk['id']\n",
    "        all_vars_in_chunk =  [(var_type, var_mention) for \n",
    "                              var_type, var_mentions in chunk['identify_vars_result'].items()\n",
    "                              for var_mention in var_mentions]\n",
    "        if len(all_vars_in_chunk) == 0: continue\n",
    "        for i in range(len(all_vars_in_chunk)):\n",
    "            for j in range(i+1, len(all_vars_in_chunk)):\n",
    "                indicator1, var1 = all_vars_in_chunk[i]\n",
    "                indicator2, var2 = all_vars_in_chunk[j]\n",
    "                links.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"var1\": var1['var'],\n",
    "                    \"var2\": var2['var'],\n",
    "                    \"indicator1\": indicator1,\n",
    "                    \"indicator2\": indicator2,\n",
    "                    \"response\": {\n",
    "                        \"relationship\": \"\",\n",
    "                        \"evidence\": \"\"\n",
    "                    }\n",
    "                })\n",
    "    return links\n",
    "chunk_w_vars = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
    "links = process_links(chunk_w_vars)\n",
    "links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:41<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
    "all_chunks = all_chunks[:10]\n",
    "candidate_links = query.filter_candidate_links(all_chunks)\n",
    "chunk_dict = {chunk['id']: dict(chunk, **{'identify_links_result':[]}) for chunk in all_chunks}\n",
    "raw_variable_definitions = json.load(open('GPTUtils/contexts/variable_definitions.json'))\n",
    "variable_definitions= {\n",
    "    var_data['var_name']: var_data['definition']\n",
    "    for var_list in raw_variable_definitions.values()\n",
    "    for var_data in var_list\n",
    "}\n",
    "\n",
    "prompt_template = json.load(open('GPTUtils/prompts/identify_links.json'))\n",
    "system_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['system_prompt_blocks']]\n",
    "user_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['user_prompt_blocks']]\n",
    "prompt_variables = {\n",
    "    \"links\": candidate_links,\n",
    "    \"variable_definitions\": variable_definitions,\n",
    "}\n",
    "prompt_list = []\n",
    "chunk_id_list = []\n",
    "link_metadata_list = []\n",
    "print(len(candidate_links))\n",
    "for link in candidate_links:\n",
    "    if link['var1'] == '其他' or link['var2'] == '其他': continue\n",
    "    conversation = chunk_dict[link['chunk_id']]['conversation']\n",
    "    prompt_variables['conversation'] = query.conversation_to_string(conversation)\n",
    "    prompt_variables['var1'] = f\"{link['var1']}, {variable_definitions[link['var1']]}\"\n",
    "    prompt_variables['var2'] = f\"{link['var2']}, {variable_definitions[link['var2']]}\"\n",
    "    prompt, response_format, extract_response_func = prompts.identify_link_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
    "    prompt_list.append(prompt)\n",
    "    chunk_id_list.append(link['chunk_id'])\n",
    "    link_metadata_list.append(link) \n",
    "responses = query.multithread_prompts(openai_client, prompt_list, response_format=response_format, temperature=0.0)\n",
    "if response_format == 'json':\n",
    "    responses = [extract_response_func(i) for i in responses]\n",
    "responses\n",
    "for (response_index, extraction_result) in enumerate(responses):\n",
    "    if extraction_result is None: continue\n",
    "    chunk_id = chunk_id_list[response_index]\n",
    "    chunk = chunk_dict[chunk_id]\n",
    "    link_metadata = link_metadata_list[response_index]\n",
    "    chunk[\"identify_links_result\"].append({\n",
    "        \"chunk_id\": link_metadata['chunk_id'],\n",
    "        \"var1\": link_metadata['var1'],\n",
    "        \"var2\": link_metadata['var2'],\n",
    "        \"indicator1\": link_metadata['indicator1'],\n",
    "        \"indicator2\": link_metadata['indicator2'],\n",
    "        \"response\": extraction_result\n",
    "    })\n",
    "all_chunks = list(chunk_dict.values())\n",
    "save_json(all_chunks, \"data/v2/user/pipeline/identify_links/chunk_w_links.json\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyudao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
