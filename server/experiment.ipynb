{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "from GPTUtils import query, prompts\n",
        "from openai import OpenAI\n",
        "def save_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "# openai (temp)\n",
        "openai_api_key = open(\"openai_api_key\").read()\n",
        "openai_client=OpenAI(api_key=openai_api_key, timeout=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables):\n",
        "#     identify_var_type_prompt_list = []\n",
        "#     response_format, extract_response_func = None, None\n",
        "#     for chunk in all_chunks:\n",
        "#         conversation = chunk['conversation']\n",
        "#         prompt_variables['conversation'] = conversation_to_string(conversation)\n",
        "#         identify_var_type_prompt, response_format, extract_response_func = prompts.identify_var_type_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "#         identify_var_type_prompt_list.append(identify_var_type_prompt)\n",
        "#     identified_var_types = query.multithread_prompts(openai_client, identify_var_type_prompt_list, response_format=response_format, temperature=0.0)\n",
        "#     if response_format == 'json':\n",
        "#         identified_var_types = [extract_response_func(i) for i in identified_var_types]\n",
        "#     for (chunk_index, var_types) in enumerate(identified_var_types):\n",
        "#         chunk = all_chunks[chunk_index]\n",
        "#         chunk['var_types'] = var_types\n",
        "#     return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# var_type_definitions = {\n",
        "#     \"driver\": \"fundamental human causes that lead to certain impacts on the environment to meet basic human needs.\",\n",
        "#     \"pressure\": \"negative phenomena or activities affecting the environment or ecosystems, which are caused by drivers or occur naturally.\",\n",
        "#     \"state\": \"the quantity and quality of physical, chemical, and biological phenomena within a specific timeframe and area.\",\n",
        "#     \"impact\": \"adverse changes in environmental conditions, ecosystem functions, or human well-being.\",\n",
        "#     \"response\": \"any behavior, action, or effort to protect the environment, address environmental issues, or be environmentally friendly.\"\n",
        "# }\n",
        "# for variable_definition_file in glob.glob(\"contexts/variable_definitions/*.json\"):\n",
        "#     variable_definitions = json.load(open(variable_definition_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt_variables = {\n",
        "#     \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "# }\n",
        "# prompts = json.load(open('prompts/identify_var_types.json'))\n",
        "# system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "# user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "# all_chunks = []\n",
        "# for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "#     chunks = json.load(open(chunk_file))\n",
        "#     all_chunks += chunks\n",
        "# # identify the variables in three prompts:\n",
        "# # First, identify if there are any mentions about the var types by extracting the sentences that mention the var types\n",
        "# # all_chunks = all_chunks[:1] # test run\n",
        "# system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "# user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "# all_chunks = identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "\n",
        "# If so, identify the corresponding variables by assigning variables to the sentences\n",
        "# If no existing variables can be matched, assign it to 'others'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = []\n",
        "for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "    chunks = json.load(open(chunk_file))\n",
        "    all_chunks += chunks\n",
        "len(all_chunks)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/init/chunks.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_var_types.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {\n",
        "    \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "}\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/init/chunks.json\"))\n",
        "print(len(all_chunks))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "\n",
        "for chunk in all_chunks:\n",
        "    var_type_result = chunk['identify_var_types_result']\n",
        "    var_type_result = list(filter(lambda x: x[\"var_type\"] != 'none', var_type_result))\n",
        "    chunk['identify_var_types_result'] = var_type_result\n",
        "\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types_filtered.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "var_definitions = {}\n",
        "for var_type in var_type_definitions.keys():\n",
        "    var_definitions_by_type = json.load(open(f\"GPTUtils/contexts/variable_definitions/{var_type}_variables_def.json\"))\n",
        "    var_definitions[var_type] = var_definitions_by_type\n",
        "\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_vars.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {}\n",
        "for var_type, var_type_def in var_type_definitions.items():\n",
        "    prompt_variables[var_type] = {\n",
        "        \"definition\": var_type_def,\n",
        "        \"vars\": \"\\n\".join([f\"{var_name}: {var_def}\" for var_name, var_def in var_definitions[var_type].items()])\n",
        "    }\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_vars/chunk_w_vars.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/init/chunks.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "chunk_conversations = [query.conversation_to_string(chunk['conversation']) for chunk in all_chunks]\n",
        "chunk_embeddings = query.multithread_embeddings(openai_client, chunk_conversations)\n",
        "res = []\n",
        "for (chunk_index, chunk_embedding) in enumerate(chunk_embeddings):\n",
        "    res.append({\n",
        "        \"id\": all_chunks[chunk_index]['id'],\n",
        "        \"embedding\": chunk_embedding\n",
        "    })\n",
        "save_json(res, \"data/v2/user/pipeline/init/chunk_embeddings.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "def process_links(chunk_w_vars):\n",
        "    links = []\n",
        "    for chunk in chunk_w_vars:\n",
        "        chunk_id = chunk['id']\n",
        "        all_vars_in_chunk =  [(var_type, var_mention) for \n",
        "                              var_type, var_mentions in chunk['identify_vars_result'].items()\n",
        "                              for var_mention in var_mentions]\n",
        "        if len(all_vars_in_chunk) == 0: continue\n",
        "        for i in range(len(all_vars_in_chunk)):\n",
        "            for j in range(i+1, len(all_vars_in_chunk)):\n",
        "                indicator1, var1 = all_vars_in_chunk[i]\n",
        "                indicator2, var2 = all_vars_in_chunk[j]\n",
        "                links.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"var1\": var1['var'],\n",
        "                    \"var2\": var2['var'],\n",
        "                    \"indicator1\": indicator1,\n",
        "                    \"indicator2\": indicator2,\n",
        "                    \"response\": {\n",
        "                        \"relationship\": \"\",\n",
        "                        \"evidence\": \"\"\n",
        "                    }\n",
        "                })\n",
        "    return links\n",
        "chunk_w_vars = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "links = process_links(chunk_w_vars)\n",
        "links[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "candidate_links = query.filter_candidate_links(all_chunks)\n",
        "chunk_dict = {chunk['id']: dict(chunk, **{'identify_links_result':[]}) for chunk in all_chunks}\n",
        "raw_variable_definitions = json.load(open('GPTUtils/contexts/variable_definitions.json'))\n",
        "variable_definitions= {\n",
        "    var_data['var_name']: var_data['definition']\n",
        "    for var_list in raw_variable_definitions.values()\n",
        "    for var_data in var_list\n",
        "}\n",
        "\n",
        "prompt_template = json.load(open('GPTUtils/prompts/identify_links.json'))\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['system_prompt_blocks']]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['user_prompt_blocks']]\n",
        "prompt_variables = {\n",
        "    \"links\": candidate_links,\n",
        "    \"variable_definitions\": variable_definitions,\n",
        "}\n",
        "prompt_list = []\n",
        "chunk_id_list = []\n",
        "link_metadata_list = []\n",
        "print(len(candidate_links))\n",
        "for link in candidate_links:\n",
        "    if link['var1'] == '其他' or link['var2'] == '其他': continue\n",
        "    conversation = chunk_dict[link['chunk_id']]['conversation']\n",
        "    prompt_variables['conversation'] = query.conversation_to_string(conversation)\n",
        "    prompt_variables['var1'] = f\"{link['var1']}, {variable_definitions[link['var1']]}\"\n",
        "    prompt_variables['var2'] = f\"{link['var2']}, {variable_definitions[link['var2']]}\"\n",
        "    prompt, response_format, extract_response_func = prompts.identify_link_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    prompt_list.append(prompt)\n",
        "    chunk_id_list.append(link['chunk_id'])\n",
        "    link_metadata_list.append(link) \n",
        "responses = query.multithread_prompts(openai_client, prompt_list, response_format=response_format, temperature=0.0)\n",
        "if response_format == 'json':\n",
        "    responses = [extract_response_func(i) for i in responses]\n",
        "responses\n",
        "for (response_index, extraction_result) in enumerate(responses):\n",
        "    if extraction_result is None: continue\n",
        "    chunk_id = chunk_id_list[response_index]\n",
        "    chunk = chunk_dict[chunk_id]\n",
        "    link_metadata = link_metadata_list[response_index]\n",
        "    chunk[\"identify_links_result\"].append({\n",
        "        \"chunk_id\": link_metadata['chunk_id'],\n",
        "        \"var1\": link_metadata['var1'],\n",
        "        \"var2\": link_metadata['var2'],\n",
        "        \"indicator1\": link_metadata['indicator1'],\n",
        "        \"indicator2\": link_metadata['indicator2'],\n",
        "        \"response\": extraction_result\n",
        "    })\n",
        "all_chunks = list(chunk_dict.values())\n",
        "save_json(all_chunks, \"data/v2/user/pipeline/identify_links/chunk_w_links.json\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "def check_evidences(results, max_index):\n",
        "    for evidences in results:\n",
        "        for evidence in evidences:\n",
        "            if evidence >= max_index:\n",
        "                return False\n",
        "    return True\n",
        "for chunk in chunks:\n",
        "    # var_type_evidences = [var_type['evidence'] for var_type in chunk['identify_var_types_result']]\n",
        "    # var_evidences = [var['evidence'] for var_list in chunk['identify_vars_result'].values() for var in var_list] \n",
        "    link_evidences = [link['response']['evidence'] for link in chunk['identify_links_result']]\n",
        "    if not check_evidences(link_evidences, len(chunk['conversation'])):\n",
        "        print(chunk['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_pairwise_jaccard(candidate_sets):\n",
        "    from itertools import combinations\n",
        "    total_jaccard_distance = 0\n",
        "    pairs = list(combinations(candidate_sets, 2))\n",
        "    for pair in pairs:\n",
        "        set1, set2 = pair\n",
        "        set1 = set(set1)\n",
        "        set2 = set(set2)\n",
        "        union = set1.union(set2)\n",
        "        if len(union) == 0: continue\n",
        "        intersection = set1.intersection(set2)\n",
        "        jaccard_distance = (len(union) - len(intersection)) / len(union)\n",
        "        total_jaccard_distance += jaccard_distance\n",
        "    return total_jaccard_distance / len(pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "def merge_var_types(var_type_results):\n",
        "    merged_var_types = {}\n",
        "    for var_types in var_type_results:\n",
        "        for var_type in var_types:\n",
        "            if var_type['var_type'] not in merged_var_types:\n",
        "                merged_var_types[var_type['var_type']] = var_type\n",
        "        else:\n",
        "            merged_var_types[var_type['var_type']]['evidence'] = list(set(merged_var_types[var_type['var_type']]['evidence'] + var_type['evidence']))\n",
        "    return list(merged_var_types.values())\n",
        "# identify var types\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append([])\n",
        "        iteration_results[chunk_index].append(chunk['identify_var_types_result'])\n",
        "for chunk_index, chunk in enumerate(all_chunks):\n",
        "    ensemble_var_types = merge_var_types(iteration_results[chunk_index])\n",
        "    candidate_var_types = list(map(lambda var_types: [x['var_type'] for x in var_types], iteration_results[chunk_index]))\n",
        "    uncertainty = average_pairwise_jaccard(candidate_var_types)\n",
        "    for var_type_result in ensemble_var_types:\n",
        "        var_type_occurrence = len(list(filter(lambda candidate: var_type_result['var_type'] in candidate, candidate_var_types)))\n",
        "        confidence = var_type_occurrence / k\n",
        "        var_type_result['uncertainty'] = uncertainty\n",
        "        var_type_result['confidence'] = confidence\n",
        "    chunk['identify_var_types_result'] = ensemble_var_types\n",
        "    if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "    chunk['uncertainty']['identify_var_types'] = uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def merge_vars(vars_results):\n",
        "    merged_vars = {}\n",
        "    for vars in vars_results:\n",
        "        for var in vars:\n",
        "            if var['var'] not in merged_vars:\n",
        "                merged_vars[var['var']] = var\n",
        "            else:\n",
        "                merged_vars[var['var']]['evidence'] = list(set(merged_vars[var['var']]['evidence'] + var['evidence']))\n",
        "    return list(merged_vars.values())\n",
        "# identify vars\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    # all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    # all_chunks = json.load(open(\"data/v2/user/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append(defaultdict(list))\n",
        "        for var_type, vars in chunk['identify_vars_result'].items():\n",
        "            # iteration_results[chunk_index][var_type].append(list(map(lambda x: x['var'], vars)))\n",
        "            iteration_results[chunk_index][var_type].append(vars)\n",
        "for chunk_index, chunk in enumerate(all_chunks[:3]):\n",
        "    for var_type, vars in iteration_results[chunk_index].items():\n",
        "        ensemble_vars = merge_vars(vars)\n",
        "        candidate_vars = list(map(lambda var_list: [x['var'] for x in var_list], vars))\n",
        "        vars_set = set([var for var_list in candidate_vars for var in var_list])\n",
        "        uncertainty = average_pairwise_jaccard(candidate_vars)\n",
        "        for var in vars_set:\n",
        "            var_occurrence = len(list(filter(lambda candidate: var in candidate, candidate_vars)))\n",
        "            confidence = var_occurrence / k\n",
        "            var_index = list(map(lambda x: x['var'], ensemble_vars)).index(var)\n",
        "            ensemble_vars[var_index]['confidence'] = confidence\n",
        "            ensemble_vars[var_index]['uncertainty'] = uncertainty\n",
        "            chunk['identify_vars_result'][var_type] = ensemble_vars\n",
        "        if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "        chunk['uncertainty']['identify_vars'] = uncertainty\n",
        "print(all_chunks[2]['identify_vars_result'])\n",
        "\n",
        "# save_json(all_chunks, \"data/v2/user/pipeline/identify_vars/chunk_w_vars_w_uncertainty.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "\n",
        "def prepare_for_finetuning(conversation, prompt):\n",
        "    conversation = merge_consecutive(conversation)\n",
        "    if len(conversation)  == 1:\n",
        "        return None\n",
        "    assert(len(conversation) > 1)\n",
        "    conversation = reformat_conversation_direct(conversation, prompt)\n",
        "    # conversations = reformat_conversation_reverse(conversation, prompt)\n",
        "    return conversation\n",
        "def merge_consecutive(conversation):\n",
        "    merged_conversation = []\n",
        "    for i, message in enumerate(conversation):\n",
        "        if i == 0:\n",
        "            merged_conversation.append(message)\n",
        "            continue\n",
        "        if conversation[i]['speaker'] == conversation[i-1]['speaker']:\n",
        "            merged_conversation[-1]['content'] += conversation[i]['content']\n",
        "        else:\n",
        "            merged_conversation.append(message)\n",
        "    return merged_conversation\n",
        "\n",
        "def reformat_conversation_direct(conversation, prompt):\n",
        "    finetune_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": prompt\n",
        "        }, \n",
        "    ]\n",
        "    for i in range(0, len(conversation), 2):\n",
        "        if i == len(conversation) - 1:\n",
        "            break\n",
        "        assert(conversation[i]['speaker'] == 1)\n",
        "        assert(conversation[i+1]['speaker'] == 0)\n",
        "        finetune_messages.append(\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conversation[i]['content']\n",
        "            })\n",
        "        finetune_messages.append(\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": conversation[i+1]['content']\n",
        "            }\n",
        "        )\n",
        "    return {\"messages\": finetune_messages}\n",
        "\n",
        "def reformat_conversations_reverse(conversation, prompt):\n",
        "    finetune_messages = []\n",
        "    for i in range(0, len(conversation), 2):\n",
        "        if i == len(conversation) - 1:\n",
        "            break\n",
        "        assert(conversation[i]['speaker'] == 1)\n",
        "        assert(conversation[i+1]['speaker'] == 0)\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": prompt\n",
        "            }, \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conversation[i]['content']\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": conversation[i+1]['content']\n",
        "            }\n",
        "        ]\n",
        "        finetune_messages.append(messages)\n",
        "    return {\"messages\": finetune_messages}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N6\n",
            "N11\n",
            "N10\n",
            "N7\n",
            "N17\n",
            "N16\n",
            "N1\n",
            "N15\n",
            "N2\n",
            "N19\n",
            "N18\n",
            "N3\n",
            "N14\n",
            "N8\n",
            "N13\n",
            "N4\n",
            "N5\n",
            "N12\n",
            "N9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "instruction_prompt = \"\"\"You are a residence of Lyudao, a small island near Taiwan. You are being interviewed about the situation on the island, ranging from environmental issues to social problems. Please answer the interview questions.\"\"\"\n",
        "for chunked_file in glob.glob(\"data/v2/user/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "    participant = chunked_file.split('/')[-1].split('.')[0]\n",
        "    print(participant)\n",
        "    chunks = json.load(open(chunked_file))\n",
        "    all_examples = []\n",
        "    for chunk in chunks:\n",
        "        conversation = chunk['conversation']\n",
        "        finetune_example = prepare_for_finetuning(conversation, instruction_prompt)\n",
        "        if finetune_example is not None:\n",
        "            all_examples.append(finetune_example)\n",
        "    Path(f\"data/finetune/{participant}\").mkdir(parents=True, exist_ok=True)\n",
        "    with open(f'data/finetune/{participant}/finetune_examples.jsonl', 'w') as outfile:\n",
        "        for example in all_examples:\n",
        "            json.dump(example, outfile, ensure_ascii=False)\n",
        "            outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a residence of Lyudao, a small island near Taiwan. You are being interviewed about the situation on the island, ranging from environmental issues to social problems. Please answer the interview questions.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"那你覺得，綠島對你來說是什麼？\"\n",
        "    }\n",
        "]\n",
        "prompts = [messages for _ in range(10)]\n",
        "# model = \"ft:gpt-4o-mini-2024-07-18:personal::9wD13Bqv\"\n",
        "participant = \"general\"\n",
        "# model = \"ft:gpt-4o-mini-2024-07-18:personal:lyudao-n5:9wFlzFxH\"\n",
        "model = \"ft:gpt-4o-mini-2024-07-18:personal:lyudao-n7:9wIU4hNG\"\n",
        "# model = \"gpt-4o-mini\"\n",
        "temperature = 0.5\n",
        "responses = query.multithread_prompts(openai_client, prompts, model=model, temperature=temperature, seed=42)\n",
        "save_json(responses, f\"data/finetune/{participant}/finetune_responses_{temperature}.json\")\n",
        "# TODO: try inserting chunks"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lyudao",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
