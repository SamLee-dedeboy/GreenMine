{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "from GPTUtils import query, prompts\n",
        "from openai import OpenAI\n",
        "def save_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "# openai (temp)\n",
        "openai_api_key = open(\"openai_api_key\").read()\n",
        "openai_client=OpenAI(api_key=openai_api_key, timeout=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables):\n",
        "#     identify_var_type_prompt_list = []\n",
        "#     response_format, extract_response_func = None, None\n",
        "#     for chunk in all_chunks:\n",
        "#         conversation = chunk['conversation']\n",
        "#         prompt_variables['conversation'] = conversation_to_string(conversation)\n",
        "#         identify_var_type_prompt, response_format, extract_response_func = prompts.identify_var_type_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "#         identify_var_type_prompt_list.append(identify_var_type_prompt)\n",
        "#     identified_var_types = query.multithread_prompts(openai_client, identify_var_type_prompt_list, response_format=response_format, temperature=0.0)\n",
        "#     if response_format == 'json':\n",
        "#         identified_var_types = [extract_response_func(i) for i in identified_var_types]\n",
        "#     for (chunk_index, var_types) in enumerate(identified_var_types):\n",
        "#         chunk = all_chunks[chunk_index]\n",
        "#         chunk['var_types'] = var_types\n",
        "#     return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# var_type_definitions = {\n",
        "#     \"driver\": \"fundamental human causes that lead to certain impacts on the environment to meet basic human needs.\",\n",
        "#     \"pressure\": \"negative phenomena or activities affecting the environment or ecosystems, which are caused by drivers or occur naturally.\",\n",
        "#     \"state\": \"the quantity and quality of physical, chemical, and biological phenomena within a specific timeframe and area.\",\n",
        "#     \"impact\": \"adverse changes in environmental conditions, ecosystem functions, or human well-being.\",\n",
        "#     \"response\": \"any behavior, action, or effort to protect the environment, address environmental issues, or be environmentally friendly.\"\n",
        "# }\n",
        "# for variable_definition_file in glob.glob(\"contexts/variable_definitions/*.json\"):\n",
        "#     variable_definitions = json.load(open(variable_definition_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt_variables = {\n",
        "#     \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "# }\n",
        "# prompts = json.load(open('prompts/identify_var_types.json'))\n",
        "# system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "# user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "# all_chunks = []\n",
        "# for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "#     chunks = json.load(open(chunk_file))\n",
        "#     all_chunks += chunks\n",
        "# # identify the variables in three prompts:\n",
        "# # First, identify if there are any mentions about the var types by extracting the sentences that mention the var types\n",
        "# # all_chunks = all_chunks[:1] # test run\n",
        "# system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "# user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "# all_chunks = identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "\n",
        "# If so, identify the corresponding variables by assigning variables to the sentences\n",
        "# If no existing variables can be matched, assign it to 'others'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = []\n",
        "for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "    chunks = json.load(open(chunk_file))\n",
        "    all_chunks += chunks\n",
        "len(all_chunks)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/init/chunks.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_var_types.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {\n",
        "    \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "}\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/init/chunks.json\"))\n",
        "print(len(all_chunks))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "\n",
        "for chunk in all_chunks:\n",
        "    var_type_result = chunk['identify_var_types_result']\n",
        "    var_type_result = list(filter(lambda x: x[\"var_type\"] != 'none', var_type_result))\n",
        "    chunk['identify_var_types_result'] = var_type_result\n",
        "\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types_filtered.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "var_definitions = {}\n",
        "for var_type in var_type_definitions.keys():\n",
        "    var_definitions_by_type = json.load(open(f\"GPTUtils/contexts/variable_definitions/{var_type}_variables_def.json\"))\n",
        "    var_definitions[var_type] = var_definitions_by_type\n",
        "\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_vars.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {}\n",
        "for var_type, var_type_def in var_type_definitions.items():\n",
        "    prompt_variables[var_type] = {\n",
        "        \"definition\": var_type_def,\n",
        "        \"vars\": \"\\n\".join([f\"{var_name}: {var_def}\" for var_name, var_def in var_definitions[var_type].items()])\n",
        "    }\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_vars/chunk_w_vars.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/init/chunks.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "chunk_conversations = [query.conversation_to_string(chunk['conversation']) for chunk in all_chunks]\n",
        "chunk_embeddings = query.multithread_embeddings(openai_client, chunk_conversations)\n",
        "res = []\n",
        "for (chunk_index, chunk_embedding) in enumerate(chunk_embeddings):\n",
        "    res.append({\n",
        "        \"id\": all_chunks[chunk_index]['id'],\n",
        "        \"embedding\": chunk_embedding\n",
        "    })\n",
        "save_json(res, \"data/v2/user/pipeline/init/chunk_embeddings.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "def process_links(chunk_w_vars):\n",
        "    links = []\n",
        "    for chunk in chunk_w_vars:\n",
        "        chunk_id = chunk['id']\n",
        "        all_vars_in_chunk =  [(var_type, var_mention) for \n",
        "                              var_type, var_mentions in chunk['identify_vars_result'].items()\n",
        "                              for var_mention in var_mentions]\n",
        "        if len(all_vars_in_chunk) == 0: continue\n",
        "        for i in range(len(all_vars_in_chunk)):\n",
        "            for j in range(i+1, len(all_vars_in_chunk)):\n",
        "                indicator1, var1 = all_vars_in_chunk[i]\n",
        "                indicator2, var2 = all_vars_in_chunk[j]\n",
        "                links.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"var1\": var1['var'],\n",
        "                    \"var2\": var2['var'],\n",
        "                    \"indicator1\": indicator1,\n",
        "                    \"indicator2\": indicator2,\n",
        "                    \"response\": {\n",
        "                        \"relationship\": \"\",\n",
        "                        \"evidence\": \"\"\n",
        "                    }\n",
        "                })\n",
        "    return links\n",
        "chunk_w_vars = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "links = process_links(chunk_w_vars)\n",
        "links[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "candidate_links = query.filter_candidate_links(all_chunks)\n",
        "chunk_dict = {chunk['id']: dict(chunk, **{'identify_links_result':[]}) for chunk in all_chunks}\n",
        "raw_variable_definitions = json.load(open('GPTUtils/contexts/variable_definitions.json'))\n",
        "variable_definitions= {\n",
        "    var_data['var_name']: var_data['definition']\n",
        "    for var_list in raw_variable_definitions.values()\n",
        "    for var_data in var_list\n",
        "}\n",
        "\n",
        "prompt_template = json.load(open('GPTUtils/prompts/identify_links.json'))\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['system_prompt_blocks']]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['user_prompt_blocks']]\n",
        "prompt_variables = {\n",
        "    \"links\": candidate_links,\n",
        "    \"variable_definitions\": variable_definitions,\n",
        "}\n",
        "prompt_list = []\n",
        "chunk_id_list = []\n",
        "link_metadata_list = []\n",
        "print(len(candidate_links))\n",
        "for link in candidate_links:\n",
        "    if link['var1'] == '其他' or link['var2'] == '其他': continue\n",
        "    conversation = chunk_dict[link['chunk_id']]['conversation']\n",
        "    prompt_variables['conversation'] = query.conversation_to_string(conversation)\n",
        "    prompt_variables['var1'] = f\"{link['var1']}, {variable_definitions[link['var1']]}\"\n",
        "    prompt_variables['var2'] = f\"{link['var2']}, {variable_definitions[link['var2']]}\"\n",
        "    prompt, response_format, extract_response_func = prompts.identify_link_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    prompt_list.append(prompt)\n",
        "    chunk_id_list.append(link['chunk_id'])\n",
        "    link_metadata_list.append(link) \n",
        "responses = query.multithread_prompts(openai_client, prompt_list, response_format=response_format, temperature=0.0)\n",
        "if response_format == 'json':\n",
        "    responses = [extract_response_func(i) for i in responses]\n",
        "responses\n",
        "for (response_index, extraction_result) in enumerate(responses):\n",
        "    if extraction_result is None: continue\n",
        "    chunk_id = chunk_id_list[response_index]\n",
        "    chunk = chunk_dict[chunk_id]\n",
        "    link_metadata = link_metadata_list[response_index]\n",
        "    chunk[\"identify_links_result\"].append({\n",
        "        \"chunk_id\": link_metadata['chunk_id'],\n",
        "        \"var1\": link_metadata['var1'],\n",
        "        \"var2\": link_metadata['var2'],\n",
        "        \"indicator1\": link_metadata['indicator1'],\n",
        "        \"indicator2\": link_metadata['indicator2'],\n",
        "        \"response\": extraction_result\n",
        "    })\n",
        "all_chunks = list(chunk_dict.values())\n",
        "save_json(all_chunks, \"data/v2/user/pipeline/identify_links/chunk_w_links.json\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "def check_evidences(results, max_index):\n",
        "    for evidences in results:\n",
        "        for evidence in evidences:\n",
        "            if evidence >= max_index:\n",
        "                return False\n",
        "    return True\n",
        "for chunk in chunks:\n",
        "    # var_type_evidences = [var_type['evidence'] for var_type in chunk['identify_var_types_result']]\n",
        "    # var_evidences = [var['evidence'] for var_list in chunk['identify_vars_result'].values() for var in var_list] \n",
        "    link_evidences = [link['response']['evidence'] for link in chunk['identify_links_result']]\n",
        "    if not check_evidences(link_evidences, len(chunk['conversation'])):\n",
        "        print(chunk['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_pairwise_jaccard(candidate_sets):\n",
        "    from itertools import combinations\n",
        "    total_jaccard_distance = 0\n",
        "    pairs = list(combinations(candidate_sets, 2))\n",
        "    for pair in pairs:\n",
        "        set1, set2 = pair\n",
        "        set1 = set(set1)\n",
        "        set2 = set(set2)\n",
        "        union = set1.union(set2)\n",
        "        if len(union) == 0: continue\n",
        "        intersection = set1.intersection(set2)\n",
        "        jaccard_distance = (len(union) - len(intersection)) / len(union)\n",
        "        total_jaccard_distance += jaccard_distance\n",
        "    return total_jaccard_distance / len(pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "def merge_var_types(var_type_results):\n",
        "    merged_var_types = {}\n",
        "    for var_types in var_type_results:\n",
        "        for var_type in var_types:\n",
        "            if var_type['var_type'] not in merged_var_types:\n",
        "                merged_var_types[var_type['var_type']] = var_type\n",
        "        else:\n",
        "            merged_var_types[var_type['var_type']]['evidence'] = list(set(merged_var_types[var_type['var_type']]['evidence'] + var_type['evidence']))\n",
        "    return list(merged_var_types.values())\n",
        "# identify var types\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append([])\n",
        "        iteration_results[chunk_index].append(chunk['identify_var_types_result'])\n",
        "for chunk_index, chunk in enumerate(all_chunks):\n",
        "    ensemble_var_types = merge_var_types(iteration_results[chunk_index])\n",
        "    candidate_var_types = list(map(lambda var_types: [x['var_type'] for x in var_types], iteration_results[chunk_index]))\n",
        "    uncertainty = average_pairwise_jaccard(candidate_var_types)\n",
        "    for var_type_result in ensemble_var_types:\n",
        "        var_type_occurrence = len(list(filter(lambda candidate: var_type_result['var_type'] in candidate, candidate_var_types)))\n",
        "        confidence = var_type_occurrence / k\n",
        "        var_type_result['uncertainty'] = uncertainty\n",
        "        var_type_result['confidence'] = confidence\n",
        "    chunk['identify_var_types_result'] = ensemble_var_types\n",
        "    if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "    chunk['uncertainty']['identify_var_types'] = uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def merge_vars(vars_results):\n",
        "    merged_vars = {}\n",
        "    for vars in vars_results:\n",
        "        for var in vars:\n",
        "            if var['var'] not in merged_vars:\n",
        "                merged_vars[var['var']] = var\n",
        "            else:\n",
        "                merged_vars[var['var']]['evidence'] = list(set(merged_vars[var['var']]['evidence'] + var['evidence']))\n",
        "    return list(merged_vars.values())\n",
        "# identify vars\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    # all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    # all_chunks = json.load(open(\"data/v2/user/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append(defaultdict(list))\n",
        "        for var_type, vars in chunk['identify_vars_result'].items():\n",
        "            # iteration_results[chunk_index][var_type].append(list(map(lambda x: x['var'], vars)))\n",
        "            iteration_results[chunk_index][var_type].append(vars)\n",
        "for chunk_index, chunk in enumerate(all_chunks[:3]):\n",
        "    for var_type, vars in iteration_results[chunk_index].items():\n",
        "        ensemble_vars = merge_vars(vars)\n",
        "        candidate_vars = list(map(lambda var_list: [x['var'] for x in var_list], vars))\n",
        "        vars_set = set([var for var_list in candidate_vars for var in var_list])\n",
        "        uncertainty = average_pairwise_jaccard(candidate_vars)\n",
        "        for var in vars_set:\n",
        "            var_occurrence = len(list(filter(lambda candidate: var in candidate, candidate_vars)))\n",
        "            confidence = var_occurrence / k\n",
        "            var_index = list(map(lambda x: x['var'], ensemble_vars)).index(var)\n",
        "            ensemble_vars[var_index]['confidence'] = confidence\n",
        "            ensemble_vars[var_index]['uncertainty'] = uncertainty\n",
        "            chunk['identify_vars_result'][var_type] = ensemble_vars\n",
        "        if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "        chunk['uncertainty']['identify_vars'] = uncertainty\n",
        "print(all_chunks[2]['identify_vars_result'])\n",
        "\n",
        "# save_json(all_chunks, \"data/v2/user/pipeline/identify_vars/chunk_w_vars_w_uncertainty.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'chunk_id': 'N1_1', 'var1': '住房', 'var2': '經濟', 'indicator1': 'driver', 'indicator2': 'driver', 'response': {'relationship': [{'relationship': 'positive', 'confidence': 1.0}], 'evidence': [1, 2, 3, 4, 8, 19], 'explanation': 'The transcript indicates a relationship between housing (住房) and the economy (經濟) through discussions about the availability of housing, land prices, and the impact of economic activities on local residents. The interviewee mentions how the demand for housing has increased due to new residents and how this has led to rising land prices, which reflects economic development. Additionally, the conversation highlights the challenges faced by local residents in accessing affordable housing and the economic implications of these issues, suggesting that housing availability is closely tied to economic conditions on the island.'}, 'uncertainty': 0.0, 'confidence': 1.0}\n",
            "{'identify_links': 0.0}\n"
          ]
        }
      ],
      "source": [
        "def merge_links(links_results):\n",
        "    merged_links = {}\n",
        "    for links in links_results:\n",
        "        for link in links:\n",
        "            link_id = f\"{link['var1']}_{link['var2']}\"\n",
        "            if link_id not in merged_links:\n",
        "                # remove after testing\n",
        "                link['response']['relationship'] = [link['response']['relationship']]\n",
        "                merged_links[link_id] = link\n",
        "            else:\n",
        "                merged_links[link_id]['response']['evidence'] = list(set(merged_links[link_id]['response']['evidence'] + link['response']['evidence']))\n",
        "                merged_links[link_id]['response']['relationship'].append(link['response']['relationship'])\n",
        "\n",
        "    return list(merged_links.values())\n",
        "\n",
        "# identify links\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append([])\n",
        "        iteration_results[chunk_index].append(chunk['identify_links_result'])\n",
        "for chunk_index, chunk in enumerate(all_chunks[:3]):\n",
        "    if chunk['identify_links_result'] == []: continue\n",
        "    ensemble_links = merge_links(iteration_results[chunk_index])\n",
        "    candidate_links = list(map(lambda links: [f\"{x['var1']}_{x['var2']}\" for x in links], iteration_results[chunk_index]))\n",
        "    uncertainty = average_pairwise_jaccard(candidate_links)\n",
        "    for link in ensemble_links:\n",
        "        link_occurrence = len(list(filter(lambda candidate: f\"{link['var1']}_{link['var2']}\" in candidate, candidate_links)))\n",
        "        confidence = link_occurrence / k\n",
        "        link['uncertainty'] = uncertainty\n",
        "        link['confidence'] = confidence\n",
        "        # relationship confidence\n",
        "        relationship_occurrences = defaultdict(int)\n",
        "        for relationship in link['response']['relationship']:\n",
        "            relationship_occurrences[relationship] += 1\n",
        "        link['response']['relationship'] = []\n",
        "        for relationship, occurrence_frequency in relationship_occurrences.items():\n",
        "            relationship_confidence = occurrence_frequency / k\n",
        "            link['response']['relationship'].append({\n",
        "                \"label\": relationship,\n",
        "                \"confidence\": relationship_confidence\n",
        "            })\n",
        "    chunk['identify_links_result'] = ensemble_links\n",
        "    if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "    chunk['uncertainty']['identify_links'] = uncertainty\n",
        "print(all_chunks[1]['identify_links_result'][0])\n",
        "print(all_chunks[1]['uncertainty'])\n",
        "# save_json(all_chunks, \"data/v2/user/pipeline/identify_links/chunk_w_links_w_uncertainty.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"answer\": {\n",
            "    \"drivers\": \"The fundamental human causes that lead to certain impacts on the environment to meet basic human needs.\",\n",
            "    \"pressures\": \"Negative phenomena or activities affecting the environment or ecosystems, which are caused by drivers or occur naturally.\",\n",
            "    \"states\": \"The quantity and quality of physical, chemical, and biological phenomena within a specific timeframe and area.\",\n",
            "    \"impacts\": \"Adverse changes in environmental conditions, ecosystem functions, or human well-being.\",\n",
            "    \"responses\": \"Any behavior, action, or effort to protect the environment, address environmental issues, or be environmentally friendly.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Answer this questions for me: What are the drivers, pressures, states, impacts, and responses in the following text? Reply with the following JSON format: {{ answer: string }}\", \n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"The drivers are the fundamental human causes that lead to certain impacts on the environment to meet basic human needs. The pressures are negative phenomena or activities affecting the environment or ecosystems, which are caused by drivers or occur naturally. The states are the quantity and quality of physical, chemical, and biological phenomena within a specific timeframe and area. The impacts are adverse changes in environmental conditions, ecosystem functions, or human well-being. The responses are any behavior, action, or effort to protect the environment, address environmental issues, or be environmentally friendly.\"\n",
        "    }\n",
        "]\n",
        "# response = query.request_gpt(openai_client, prompts, \"gpt-4o-mini\", 0.0, \"json\", None)\n",
        "response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=prompts,\n",
        "        # response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.0,\n",
        "        seed=42,\n",
        "    ),\n",
        "\n",
        "print(response[0].choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "\n",
        "def prepare_for_finetuning(conversation, prompt):\n",
        "    conversation = merge_consecutive(conversation)\n",
        "    if len(conversation)  == 1:\n",
        "        return None\n",
        "    assert(len(conversation) > 1)\n",
        "    conversation = reformat_conversation_direct(conversation, prompt)\n",
        "    # conversations = reformat_conversation_reverse(conversation, prompt)\n",
        "    return conversation\n",
        "def merge_consecutive(conversation):\n",
        "    merged_conversation = []\n",
        "    for i, message in enumerate(conversation):\n",
        "        if i == 0:\n",
        "            merged_conversation.append(message)\n",
        "            continue\n",
        "        if conversation[i]['speaker'] == conversation[i-1]['speaker']:\n",
        "            merged_conversation[-1]['content'] += conversation[i]['content']\n",
        "        else:\n",
        "            merged_conversation.append(message)\n",
        "    return merged_conversation\n",
        "\n",
        "def reformat_conversation_direct(conversation, prompt):\n",
        "    finetune_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": prompt\n",
        "        }, \n",
        "    ]\n",
        "    for i in range(0, len(conversation), 2):\n",
        "        if i == len(conversation) - 1:\n",
        "            break\n",
        "        assert(conversation[i]['speaker'] == 1)\n",
        "        assert(conversation[i+1]['speaker'] == 0)\n",
        "        finetune_messages.append(\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conversation[i]['content']\n",
        "            })\n",
        "        finetune_messages.append(\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": conversation[i+1]['content']\n",
        "            }\n",
        "        )\n",
        "    return {\"messages\": finetune_messages}\n",
        "\n",
        "def reformat_conversations_reverse(conversation, prompt):\n",
        "    finetune_messages = []\n",
        "    for i in range(0, len(conversation), 2):\n",
        "        if i == len(conversation) - 1:\n",
        "            break\n",
        "        assert(conversation[i]['speaker'] == 1)\n",
        "        assert(conversation[i+1]['speaker'] == 0)\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": prompt\n",
        "            }, \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conversation[i]['content']\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": conversation[i+1]['content']\n",
        "            }\n",
        "        ]\n",
        "        finetune_messages.append(messages)\n",
        "    return {\"messages\": finetune_messages}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "instruction_prompt = \"\"\"You are a residence of Lyudao, a small island near Taiwan. You are being interviewed about the situation on the island, ranging from environmental issues to social problems. Please answer the interview questions.\"\"\"\n",
        "for chunked_file in glob.glob(\"data/v2/user/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "    participant = chunked_file.split('/')[-1].split('.')[0]\n",
        "    print(participant)\n",
        "    chunks = json.load(open(chunked_file))\n",
        "    all_examples = []\n",
        "    for chunk in chunks:\n",
        "        conversation = chunk['conversation']\n",
        "        finetune_example = prepare_for_finetuning(conversation, instruction_prompt)\n",
        "        if finetune_example is not None:\n",
        "            all_examples.append(finetune_example)\n",
        "    Path(f\"data/finetune/{participant}\").mkdir(parents=True, exist_ok=True)\n",
        "    with open(f'data/finetune/{participant}/finetune_examples.jsonl', 'w') as outfile:\n",
        "        for example in all_examples:\n",
        "            json.dump(example, outfile, ensure_ascii=False)\n",
        "            outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a residence of Lyudao, a small island near Taiwan. You are being interviewed about the situation on the island, ranging from environmental issues to social problems. Please answer the interview questions.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"那你覺得，綠島對你來說是什麼？\"\n",
        "    }\n",
        "]\n",
        "prompts = [messages for _ in range(10)]\n",
        "# model = \"ft:gpt-4o-mini-2024-07-18:personal::9wD13Bqv\"\n",
        "participant = \"general\"\n",
        "# model = \"ft:gpt-4o-mini-2024-07-18:personal:lyudao-n5:9wFlzFxH\"\n",
        "model = \"ft:gpt-4o-mini-2024-07-18:personal:lyudao-n7:9wIU4hNG\"\n",
        "# model = \"gpt-4o-mini\"\n",
        "temperature = 0.5\n",
        "responses = query.multithread_prompts(openai_client, prompts, model=model, temperature=temperature, seed=42)\n",
        "save_json(responses, f\"data/finetune/{participant}/finetune_responses_{temperature}.json\")\n",
        "# TODO: try inserting chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from DataUtils import dr\n",
        "from collections import defaultdict\n",
        "import jieba\n",
        "\n",
        "\n",
        "def generate_DPSIR_data(chunks, var_type_mentions, variable_definitions, keyword_embeddings, stopwords=[], userdict=None):\n",
        "    chunks_dict = {chunk['id']: chunk for chunk in chunks}\n",
        "    for var_type, variables in variable_definitions.items():\n",
        "        variables = {var_data['var_name']: var_data for var_data in variables}\n",
        "        variable_definitions[var_type] = variables\n",
        "    res = {}\n",
        "    mentioned_chunk_ids = set()\n",
        "    for var_type, var_type_data in var_type_mentions.items():\n",
        "        res[var_type] = {\n",
        "            \"variable_type\": var_type,\n",
        "            \"variable_mentions\": {},\n",
        "            \"keyword_data\": {\n",
        "                \"keyword_list\": [],\n",
        "                \"keyword_coordinates\": {},\n",
        "                \"keyword_statistics\": {}\n",
        "            }\n",
        "        }\n",
        "        for variable_data in var_type_data['variable_mentions'].values():\n",
        "            variable_name = variable_data['variable_name']\n",
        "            if variable_name not in res[var_type]['variable_mentions']:\n",
        "                res[var_type]['variable_mentions'][variable_name] = {\n",
        "                    \"variable_name\": variable_name,\n",
        "                    \"definition\": variable_definitions[var_type][variable_name]['definition'] if variable_name in variable_definitions[var_type] else \"unknown\",\n",
        "                    \"factor_type\": variable_definitions[var_type][variable_name]['factor_type'] if variable_name in variable_definitions[var_type] else \"unknown\",\n",
        "                    \"mentions\": [],\n",
        "                }\n",
        "            res[var_type]['variable_mentions'][variable_name]['mentions'] += variable_data['mentions']\n",
        "            mentioned_chunk_ids.update(list(map(lambda x: x['chunk_id'], variable_data['mentions'])))\n",
        "        mentioned_chunk_data = [chunks_dict[chunk_id] for chunk_id in mentioned_chunk_ids]\n",
        "        keyword_list, keyword_statistics, keyword_coordinates = generate_keyword_data(var_type, mentioned_chunk_data, keyword_embeddings, stopwords, userdict)\n",
        "        res[var_type]['keyword_data'] = {\n",
        "            \"keyword_list\": keyword_list,\n",
        "            \"keyword_statistics\": keyword_statistics,\n",
        "            \"keyword_coordinates\": keyword_coordinates\n",
        "        }\n",
        "    return res\n",
        "\n",
        "def generate_keyword_data(var_type, chunks, keyword_embeddings, stopwords=[], userdict=None):\n",
        "\n",
        "    keyword_embeddings_dict = {keyword['keyword']: keyword for keyword in keyword_embeddings}\n",
        "    all_keywords = set()\n",
        "    keyword_statistics = defaultdict(int)\n",
        "    keyword_coordinates = {}\n",
        "    for chunk in chunks:\n",
        "        if chunk['identify_var_types_result'] == []: continue\n",
        "        evidences = list(set([sentence_index for evidences in map(lambda x: x['evidence'], filter(lambda y: y['var_type'] == var_type, chunk['identify_var_types_result'])) for sentence_index in evidences]))\n",
        "        print(evidences)\n",
        "        evidence_messages = [chunk['conversation'][i]['content'] for i in evidences]\n",
        "        chunk_keywords = set()\n",
        "        for sentence in evidence_messages:\n",
        "            words = jieba.cut(sentence)\n",
        "            words = list(filter(lambda x: x not in stopwords, words))\n",
        "            words = list(filter(lambda x: x in keyword_embeddings_dict, words))\n",
        "            chunk_keywords.update(words)\n",
        "        for keyword in chunk_keywords:\n",
        "            keyword_statistics[keyword] += 1\n",
        "        all_keywords.update(chunk_keywords)\n",
        "    all_keywords = list(all_keywords) \n",
        "    keyword_statistics = {\n",
        "        keyword: {\n",
        "            \"frequency\": freq\n",
        "            } \n",
        "        for keyword, freq in keyword_statistics.items()\n",
        "    }\n",
        "    # all_keyword_embeddings = [keyword_embeddings_dict[keyword]['embedding'] for keyword in all_keywords] \n",
        "    # XY = dr.scatter_plot(all_keyword_embeddings)\n",
        "    # for keyword, coordinate in zip(all_keywords, XY):\n",
        "    #     keyword_coordinates[keyword] = coordinate.tolist()\n",
        "    return all_keywords, keyword_statistics, keyword_coordinates\n",
        "\n",
        "nodes = {}\n",
        "var_types = [\"driver\", \"pressure\", \"state\", \"impact\", \"response\"]\n",
        "node_data_path = \"data/v2/user/nodes/\"\n",
        "for var_type in var_types:\n",
        "    nodes[var_type] = json.load(\n",
        "        open(node_data_path + f\"{var_type}_nodes.json\", encoding=\"utf-8\")\n",
        "    )\n",
        "variable_definitions = json.load(\n",
        "    open(\"GPTUtils/contexts/variable_definitions.json\", encoding=\"utf-8\")\n",
        ")\n",
        "chunks = json.load(open(\"data/v2/user/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "keyword_embeddings = json.load(open(\"data/v2/user/keyword/keywords.json\"))\n",
        "stopwords = [\"綠島\"]\n",
        "userdict = \"data/v2/user/keyword/userdict.txt\"\n",
        "res = generate_DPSIR_data(chunks, nodes, variable_definitions, keyword_embeddings, stopwords, userdict)\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords = json.load(open(\"data/v2/user/keyword/keywords.json\"))\n",
        "keyword_list = [keyword['keyword'] for keyword in keywords]\n",
        "# write keyword to a txt file, each line is a keyword\n",
        "with open(\"data/v2/user/keyword/userdict.txt\", \"w\") as f:\n",
        "    for keyword in keyword_list:\n",
        "        f.write(f\"{keyword}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keyword_embeddings = json.load(open(\"data/v2/user/keyword/keywords.json\"))\n",
        "kpca_reducer = dr.init_kpca_reducer(\n",
        "    list(map(lambda x: x[\"embedding\"], keyword_embeddings))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "for chunk in all_chunks:\n",
        "    if \"identify_vars\" not in chunk['uncertainty']:\n",
        "        chunk['uncertainty']['identify_vars'] = 0.0\n",
        "    if \"identify_links\" not in chunk['uncertainty']:\n",
        "        chunk['uncertainty']['identify_links'] = 0.0\n",
        "save_json(all_chunks, \"data/v2/user/pipeline/identify_links/chunk_w_links.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "translation_prompt = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a translator. Translate the user provided text into Traditional Chinese.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": text\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "for chunk in all_chunks:\n",
        "    identify_vars_result = chunk['identify_vars_result']\n",
        "    for var_type, var_list in identify_vars_result.items():\n",
        "        if var_type == \"driver\":\n",
        "            for var in var_list:\n",
        "                if var['var'] == \"土地利用和土地覆蓋變化\":\n",
        "                    print(chunk['id'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lyudao",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
