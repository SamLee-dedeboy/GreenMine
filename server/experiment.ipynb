{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "from GPTUtils import query, prompts\n",
        "from openai import OpenAI\n",
        "def save_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "# openai (temp)\n",
        "openai_api_key = open(\"openai_api_key\").read()\n",
        "openai_client=OpenAI(api_key=openai_api_key, timeout=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables):\n",
        "#     identify_var_type_prompt_list = []\n",
        "#     response_format, extract_response_func = None, None\n",
        "#     for chunk in all_chunks:\n",
        "#         conversation = chunk['conversation']\n",
        "#         prompt_variables['conversation'] = conversation_to_string(conversation)\n",
        "#         identify_var_type_prompt, response_format, extract_response_func = prompts.identify_var_type_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "#         identify_var_type_prompt_list.append(identify_var_type_prompt)\n",
        "#     identified_var_types = query.multithread_prompts(openai_client, identify_var_type_prompt_list, response_format=response_format, temperature=0.0)\n",
        "#     if response_format == 'json':\n",
        "#         identified_var_types = [extract_response_func(i) for i in identified_var_types]\n",
        "#     for (chunk_index, var_types) in enumerate(identified_var_types):\n",
        "#         chunk = all_chunks[chunk_index]\n",
        "#         chunk['var_types'] = var_types\n",
        "#     return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# var_type_definitions = {\n",
        "#     \"driver\": \"fundamental human causes that lead to certain impacts on the environment to meet basic human needs.\",\n",
        "#     \"pressure\": \"negative phenomena or activities affecting the environment or ecosystems, which are caused by drivers or occur naturally.\",\n",
        "#     \"state\": \"the quantity and quality of physical, chemical, and biological phenomena within a specific timeframe and area.\",\n",
        "#     \"impact\": \"adverse changes in environmental conditions, ecosystem functions, or human well-being.\",\n",
        "#     \"response\": \"any behavior, action, or effort to protect the environment, address environmental issues, or be environmentally friendly.\"\n",
        "# }\n",
        "# for variable_definition_file in glob.glob(\"contexts/variable_definitions/*.json\"):\n",
        "#     variable_definitions = json.load(open(variable_definition_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt_variables = {\n",
        "#     \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "# }\n",
        "# prompts = json.load(open('prompts/identify_var_types.json'))\n",
        "# system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "# user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "# all_chunks = []\n",
        "# for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "#     chunks = json.load(open(chunk_file))\n",
        "#     all_chunks += chunks\n",
        "# # identify the variables in three prompts:\n",
        "# # First, identify if there are any mentions about the var types by extracting the sentences that mention the var types\n",
        "# # all_chunks = all_chunks[:1] # test run\n",
        "# system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "# user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "# all_chunks = identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "\n",
        "# If so, identify the corresponding variables by assigning variables to the sentences\n",
        "# If no existing variables can be matched, assign it to 'others'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/v2/tmp/pipeline/init/chunks.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     all_chunks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunks\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mlen\u001b[39m(all_chunks)\n\u001b[0;32m----> 6\u001b[0m save_json(all_chunks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/v2/tmp/pipeline/init/chunks.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36msave_json\u001b[0;34m(data, filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_json\u001b[39m(data, filename):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/lyudao/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/v2/tmp/pipeline/init/chunks.json'"
          ]
        }
      ],
      "source": [
        "all_chunks = []\n",
        "for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "    chunks = json.load(open(chunk_file))\n",
        "    all_chunks += chunks\n",
        "len(all_chunks)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/init/chunks.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 598/598 [00:20<00:00, 28.84it/s]\n"
          ]
        }
      ],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_var_types.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {\n",
        "    \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "}\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/init/chunks.json\"))\n",
        "print(len(all_chunks))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "\n",
        "for chunk in all_chunks:\n",
        "    var_type_result = chunk['identify_var_types_result']\n",
        "    var_type_result = list(filter(lambda x: x[\"var_type\"] != 'none', var_type_result))\n",
        "    chunk['identify_var_types_result'] = var_type_result\n",
        "\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types_filtered.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 774/774 [00:36<00:00, 20.99it/s]\n"
          ]
        }
      ],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "var_definitions = {}\n",
        "for var_type in var_type_definitions.keys():\n",
        "    var_definitions_by_type = json.load(open(f\"GPTUtils/contexts/variable_definitions/{var_type}_variables_def.json\"))\n",
        "    var_definitions[var_type] = var_definitions_by_type\n",
        "\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_vars.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {}\n",
        "for var_type, var_type_def in var_type_definitions.items():\n",
        "    prompt_variables[var_type] = {\n",
        "        \"definition\": var_type_def,\n",
        "        \"vars\": \"\\n\".join([f\"{var_name}: {var_def}\" for var_name, var_def in var_definitions[var_type].items()])\n",
        "    }\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_vars/chunk_w_vars.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: tokens:  322 False\n",
            "tokens:  255 False\n",
            " 96 False\n",
            "tokens:  468 False\n",
            "tokens:  128 False\n",
            "tokens:  404 False\n",
            "tokens:  431 False\n",
            "tokens:  284 False\n",
            "tokens:  84 False\n",
            "tokens:  267 False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:02<00:00,  4.21it/s]\n"
          ]
        }
      ],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/init/chunks.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "chunk_conversations = [query.conversation_to_string(chunk['conversation']) for chunk in all_chunks]\n",
        "chunk_embeddings = query.multithread_embeddings(openai_client, chunk_conversations)\n",
        "res = []\n",
        "for (chunk_index, chunk_embedding) in enumerate(chunk_embeddings):\n",
        "    res.append({\n",
        "        \"id\": all_chunks[chunk_index]['id'],\n",
        "        \"embedding\": chunk_embedding\n",
        "    })\n",
        "save_json(res, \"data/v2/user/pipeline/init/chunk_embeddings.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'chunk_id': 'N6_3',\n",
              " 'var1': '旅遊業',\n",
              " 'var2': '人口',\n",
              " 'indicator1': 'driver',\n",
              " 'indicator2': 'driver',\n",
              " 'response': {'relationship': '', 'evidence': ''}}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "def process_links(chunk_w_vars):\n",
        "    links = []\n",
        "    for chunk in chunk_w_vars:\n",
        "        chunk_id = chunk['id']\n",
        "        all_vars_in_chunk =  [(var_type, var_mention) for \n",
        "                              var_type, var_mentions in chunk['identify_vars_result'].items()\n",
        "                              for var_mention in var_mentions]\n",
        "        if len(all_vars_in_chunk) == 0: continue\n",
        "        for i in range(len(all_vars_in_chunk)):\n",
        "            for j in range(i+1, len(all_vars_in_chunk)):\n",
        "                indicator1, var1 = all_vars_in_chunk[i]\n",
        "                indicator2, var2 = all_vars_in_chunk[j]\n",
        "                links.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"var1\": var1['var'],\n",
        "                    \"var2\": var2['var'],\n",
        "                    \"indicator1\": indicator1,\n",
        "                    \"indicator2\": indicator2,\n",
        "                    \"response\": {\n",
        "                        \"relationship\": \"\",\n",
        "                        \"evidence\": \"\"\n",
        "                    }\n",
        "                })\n",
        "    return links\n",
        "chunk_w_vars = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "links = process_links(chunk_w_vars)\n",
        "links[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 107/107 [00:41<00:00,  2.57it/s]\n"
          ]
        }
      ],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "candidate_links = query.filter_candidate_links(all_chunks)\n",
        "chunk_dict = {chunk['id']: dict(chunk, **{'identify_links_result':[]}) for chunk in all_chunks}\n",
        "raw_variable_definitions = json.load(open('GPTUtils/contexts/variable_definitions.json'))\n",
        "variable_definitions= {\n",
        "    var_data['var_name']: var_data['definition']\n",
        "    for var_list in raw_variable_definitions.values()\n",
        "    for var_data in var_list\n",
        "}\n",
        "\n",
        "prompt_template = json.load(open('GPTUtils/prompts/identify_links.json'))\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['system_prompt_blocks']]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['user_prompt_blocks']]\n",
        "prompt_variables = {\n",
        "    \"links\": candidate_links,\n",
        "    \"variable_definitions\": variable_definitions,\n",
        "}\n",
        "prompt_list = []\n",
        "chunk_id_list = []\n",
        "link_metadata_list = []\n",
        "print(len(candidate_links))\n",
        "for link in candidate_links:\n",
        "    if link['var1'] == '其他' or link['var2'] == '其他': continue\n",
        "    conversation = chunk_dict[link['chunk_id']]['conversation']\n",
        "    prompt_variables['conversation'] = query.conversation_to_string(conversation)\n",
        "    prompt_variables['var1'] = f\"{link['var1']}, {variable_definitions[link['var1']]}\"\n",
        "    prompt_variables['var2'] = f\"{link['var2']}, {variable_definitions[link['var2']]}\"\n",
        "    prompt, response_format, extract_response_func = prompts.identify_link_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    prompt_list.append(prompt)\n",
        "    chunk_id_list.append(link['chunk_id'])\n",
        "    link_metadata_list.append(link) \n",
        "responses = query.multithread_prompts(openai_client, prompt_list, response_format=response_format, temperature=0.0)\n",
        "if response_format == 'json':\n",
        "    responses = [extract_response_func(i) for i in responses]\n",
        "responses\n",
        "for (response_index, extraction_result) in enumerate(responses):\n",
        "    if extraction_result is None: continue\n",
        "    chunk_id = chunk_id_list[response_index]\n",
        "    chunk = chunk_dict[chunk_id]\n",
        "    link_metadata = link_metadata_list[response_index]\n",
        "    chunk[\"identify_links_result\"].append({\n",
        "        \"chunk_id\": link_metadata['chunk_id'],\n",
        "        \"var1\": link_metadata['var1'],\n",
        "        \"var2\": link_metadata['var2'],\n",
        "        \"indicator1\": link_metadata['indicator1'],\n",
        "        \"indicator2\": link_metadata['indicator2'],\n",
        "        \"response\": extraction_result\n",
        "    })\n",
        "all_chunks = list(chunk_dict.values())\n",
        "save_json(all_chunks, \"data/v2/user/pipeline/identify_links/chunk_w_links.json\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "def check_evidences(results, max_index):\n",
        "    for evidences in results:\n",
        "        for evidence in evidences:\n",
        "            if evidence >= max_index:\n",
        "                return False\n",
        "    return True\n",
        "for chunk in chunks:\n",
        "    # var_type_evidences = [var_type['evidence'] for var_type in chunk['identify_var_types_result']]\n",
        "    # var_evidences = [var['evidence'] for var_list in chunk['identify_vars_result'].values() for var in var_list] \n",
        "    link_evidences = [link['response']['evidence'] for link in chunk['identify_links_result']]\n",
        "    if not check_evidences(link_evidences, len(chunk['conversation'])):\n",
        "        print(chunk['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_pairwise_jaccard(candidate_sets):\n",
        "    from itertools import combinations\n",
        "    total_jaccard_distance = 0\n",
        "    pairs = list(combinations(candidate_sets, 2))\n",
        "    for pair in pairs:\n",
        "        set1, set2 = pair\n",
        "        set1 = set(set1)\n",
        "        set2 = set(set2)\n",
        "        union = set1.union(set2)\n",
        "        if len(union) == 0: continue\n",
        "        intersection = set1.intersection(set2)\n",
        "        jaccard_distance = (len(union) - len(intersection)) / len(union)\n",
        "        total_jaccard_distance += jaccard_distance\n",
        "    return total_jaccard_distance / len(pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'var_type': 'driver', 'evidence': [1, 2, 3, 4, 7], 'explanation': 'The interviewee discusses the fundamental human need for housing and the economic activities surrounding it, such as renting and utilizing vacant spaces. This indicates that the drivers of these issues are related to the basic human need for shelter and the economic motivations of the residents.', 'uncertainty': 0.0, 'confidence': 1.0}, {'var_type': 'pressure', 'evidence': [3, 4, 15, 18], 'explanation': 'The discussion includes pressures on the environment and local resources, such as the overdevelopment of land and the strain on public services like transportation and parking. These pressures are caused by the actions of residents and businesses in response to economic opportunities.', 'uncertainty': 0.0, 'confidence': 1.0}, {'var_type': 'state', 'evidence': [0, 1, 9, 10], 'explanation': \"The interviewee describes the current state of the island's economy, housing situation, and community dynamics, indicating the quality and quantity of resources available to residents and the overall living conditions.\", 'uncertainty': 0.0, 'confidence': 1.0}, {'var_type': 'impact', 'evidence': [9, 10, 14, 19], 'explanation': 'The interviewee mentions adverse changes in the local economy and the quality of life for residents, such as higher prices for goods and the exploitation of residents by businesses. This reflects the negative impacts on human well-being and the local ecosystem.', 'uncertainty': 0.0, 'confidence': 1.0}, {'var_type': 'response', 'evidence': [8, 16, 7], 'explanation': 'The interviewee talks about local initiatives aimed at revitalizing the community and attracting young people back to the island, such as cultural projects and discussions about improving local services. These actions represent responses to the challenges faced by the community.', 'uncertainty': 0.0, 'confidence': 1.0}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def merge_var_types(var_type_results):\n",
        "    merged_var_types = {}\n",
        "    for var_types in var_type_results:\n",
        "        for var_type in var_types:\n",
        "            if var_type['var_type'] not in merged_var_types:\n",
        "                merged_var_types[var_type['var_type']] = var_type\n",
        "        else:\n",
        "            merged_var_types[var_type['var_type']]['evidence'] = list(set(merged_var_types[var_type['var_type']]['evidence'] + var_type['evidence']))\n",
        "    return list(merged_var_types.values())\n",
        "# identify var types\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append([])\n",
        "        iteration_results[chunk_index].append(chunk['identify_var_types_result'])\n",
        "for chunk_index, chunk in enumerate(all_chunks):\n",
        "    ensemble_var_types = merge_var_types(iteration_results[chunk_index])\n",
        "    candidate_var_types = list(map(lambda var_types: [x['var_type'] for x in var_types], iteration_results[chunk_index]))\n",
        "    uncertainty = average_pairwise_jaccard(candidate_var_types)\n",
        "    for var_type_result in ensemble_var_types:\n",
        "        var_type_occurrence = len(list(filter(lambda candidate: var_type_result['var_type'] in candidate, candidate_var_types)))\n",
        "        confidence = var_type_occurrence / k\n",
        "        var_type_result['uncertainty'] = uncertainty\n",
        "        var_type_result['confidence'] = confidence\n",
        "    chunk['identify_var_types_result'] = ensemble_var_types\n",
        "    if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "    chunk['uncertainty']['identify_var_types'] = uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'pressure': [{'var': '土地利用和土地覆蓋變化', 'evidence': [0, 1, 3], 'explanation': 'The interviewee discusses the contradictions of applying for water and electricity in a conservation area, which implies land use changes and development activities that are not in line with conservation efforts. This indicates pressure on the environment due to human activities.', 'confidence': 1.0, 'uncertainty': 0.0}]}\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "def merge_vars(vars_results):\n",
        "    merged_vars = {}\n",
        "    for vars in vars_results:\n",
        "        for var in vars:\n",
        "            if var['var'] not in merged_vars:\n",
        "                merged_vars[var['var']] = var\n",
        "            else:\n",
        "                merged_vars[var['var']]['evidence'] = list(set(merged_vars[var['var']]['evidence'] + var['evidence']))\n",
        "    return list(merged_vars.values())\n",
        "# identify vars\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    # all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    # all_chunks = json.load(open(\"data/v2/user/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append(defaultdict(list))\n",
        "        for var_type, vars in chunk['identify_vars_result'].items():\n",
        "            # iteration_results[chunk_index][var_type].append(list(map(lambda x: x['var'], vars)))\n",
        "            iteration_results[chunk_index][var_type].append(vars)\n",
        "for chunk_index, chunk in enumerate(all_chunks[:3]):\n",
        "    for var_type, vars in iteration_results[chunk_index].items():\n",
        "        ensemble_vars = merge_vars(vars)\n",
        "        candidate_vars = list(map(lambda var_list: [x['var'] for x in var_list], vars))\n",
        "        vars_set = set([var for var_list in candidate_vars for var in var_list])\n",
        "        uncertainty = average_pairwise_jaccard(candidate_vars)\n",
        "        for var in vars_set:\n",
        "            var_occurrence = len(list(filter(lambda candidate: var in candidate, candidate_vars)))\n",
        "            confidence = var_occurrence / k\n",
        "            var_index = list(map(lambda x: x['var'], ensemble_vars)).index(var)\n",
        "            ensemble_vars[var_index]['confidence'] = confidence\n",
        "            ensemble_vars[var_index]['uncertainty'] = uncertainty\n",
        "            chunk['identify_vars_result'][var_type] = ensemble_vars\n",
        "        if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "        chunk['uncertainty']['identify_vars'] = uncertainty\n",
        "print(all_chunks[2]['identify_vars_result'])\n",
        "\n",
        "# save_json(all_chunks, \"data/v2/user/pipeline/identify_vars/chunk_w_vars_w_uncertainty.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_var_types/chunk_w_var_types_w_uncertainty.json\"))\n",
        "uncertainties = [chunk['uncertainty_var_type'] for chunk in all_chunks]\n",
        "# visualize the uncertainty using matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.hist(uncertainties, bins=20)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lyudao",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
