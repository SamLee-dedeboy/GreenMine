{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "from GPTUtils import query, prompts\n",
        "from openai import OpenAI\n",
        "def save_json(data, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "# openai (temp)\n",
        "openai_api_key = open(\"openai_api_key\").read()\n",
        "openai_client=OpenAI(api_key=openai_api_key, timeout=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables):\n",
        "#     identify_var_type_prompt_list = []\n",
        "#     response_format, extract_response_func = None, None\n",
        "#     for chunk in all_chunks:\n",
        "#         conversation = chunk['conversation']\n",
        "#         prompt_variables['conversation'] = conversation_to_string(conversation)\n",
        "#         identify_var_type_prompt, response_format, extract_response_func = prompts.identify_var_type_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "#         identify_var_type_prompt_list.append(identify_var_type_prompt)\n",
        "#     identified_var_types = query.multithread_prompts(openai_client, identify_var_type_prompt_list, response_format=response_format, temperature=0.0)\n",
        "#     if response_format == 'json':\n",
        "#         identified_var_types = [extract_response_func(i) for i in identified_var_types]\n",
        "#     for (chunk_index, var_types) in enumerate(identified_var_types):\n",
        "#         chunk = all_chunks[chunk_index]\n",
        "#         chunk['var_types'] = var_types\n",
        "#     return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# var_type_definitions = {\n",
        "#     \"driver\": \"fundamental human causes that lead to certain impacts on the environment to meet basic human needs.\",\n",
        "#     \"pressure\": \"negative phenomena or activities affecting the environment or ecosystems, which are caused by drivers or occur naturally.\",\n",
        "#     \"state\": \"the quantity and quality of physical, chemical, and biological phenomena within a specific timeframe and area.\",\n",
        "#     \"impact\": \"adverse changes in environmental conditions, ecosystem functions, or human well-being.\",\n",
        "#     \"response\": \"any behavior, action, or effort to protect the environment, address environmental issues, or be environmentally friendly.\"\n",
        "# }\n",
        "# for variable_definition_file in glob.glob(\"contexts/variable_definitions/*.json\"):\n",
        "#     variable_definitions = json.load(open(variable_definition_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt_variables = {\n",
        "#     \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "# }\n",
        "# prompts = json.load(open('prompts/identify_var_types.json'))\n",
        "# system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "# user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "# all_chunks = []\n",
        "# for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "#     chunks = json.load(open(chunk_file))\n",
        "#     all_chunks += chunks\n",
        "# # identify the variables in three prompts:\n",
        "# # First, identify if there are any mentions about the var types by extracting the sentences that mention the var types\n",
        "# # all_chunks = all_chunks[:1] # test run\n",
        "# system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "# user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "# all_chunks = identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "\n",
        "# If so, identify the corresponding variables by assigning variables to the sentences\n",
        "# If no existing variables can be matched, assign it to 'others'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = []\n",
        "for chunk_file in glob.glob(\"data/v2/tmp/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "    chunks = json.load(open(chunk_file))\n",
        "    all_chunks += chunks\n",
        "len(all_chunks)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/init/chunks.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_var_types.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {\n",
        "    \"var_types\": \"\\n\".join([f\"{var_type}: {var_type_def}\" for var_type, var_type_def in var_type_definitions.items()]),\n",
        "}\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/init/chunks.json\"))\n",
        "print(len(all_chunks))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_var_types(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "\n",
        "for chunk in all_chunks:\n",
        "    var_type_result = chunk['identify_var_types_result']\n",
        "    var_type_result = list(filter(lambda x: x[\"var_type\"] != 'none', var_type_result))\n",
        "    chunk['identify_var_types_result'] = var_type_result\n",
        "\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types_filtered.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var_type_definitions = json.load(open('GPTUtils/contexts/var_type_definitions.json'))\n",
        "var_definitions = {}\n",
        "for var_type in var_type_definitions.keys():\n",
        "    var_definitions_by_type = json.load(open(f\"GPTUtils/contexts/variable_definitions/{var_type}_variables_def.json\"))\n",
        "    var_definitions[var_type] = var_definitions_by_type\n",
        "\n",
        "prompts = json.load(open('GPTUtils/prompts/identify_vars.json'))\n",
        "system_prompt_blocks = prompts['system_prompt_blocks']\n",
        "user_prompt_blocks = prompts['user_prompt_blocks']\n",
        "prompt_variables = {}\n",
        "for var_type, var_type_def in var_type_definitions.items():\n",
        "    prompt_variables[var_type] = {\n",
        "        \"definition\": var_type_def,\n",
        "        \"vars\": \"\\n\".join([f\"{var_name}: {var_def}\" for var_name, var_def in var_definitions[var_type].items()])\n",
        "    }\n",
        "all_chunks = json.load(open(\"data/v2/tmp/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "# all_chunks = all_chunks[:10]\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in system_prompt_blocks]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in user_prompt_blocks]\n",
        "all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "save_json(all_chunks, \"data/v2/tmp/pipeline/identify_vars/chunk_w_vars.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/init/chunks.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "chunk_conversations = [query.conversation_to_string(chunk['conversation']) for chunk in all_chunks]\n",
        "chunk_embeddings = query.multithread_embeddings(openai_client, chunk_conversations)\n",
        "res = []\n",
        "for (chunk_index, chunk_embedding) in enumerate(chunk_embeddings):\n",
        "    res.append({\n",
        "        \"id\": all_chunks[chunk_index]['id'],\n",
        "        \"embedding\": chunk_embedding\n",
        "    })\n",
        "save_json(res, \"data/v2/user/pipeline/init/chunk_embeddings.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "def process_links(chunk_w_vars):\n",
        "    links = []\n",
        "    for chunk in chunk_w_vars:\n",
        "        chunk_id = chunk['id']\n",
        "        all_vars_in_chunk =  [(var_type, var_mention) for \n",
        "                              var_type, var_mentions in chunk['identify_vars_result'].items()\n",
        "                              for var_mention in var_mentions]\n",
        "        if len(all_vars_in_chunk) == 0: continue\n",
        "        for i in range(len(all_vars_in_chunk)):\n",
        "            for j in range(i+1, len(all_vars_in_chunk)):\n",
        "                indicator1, var1 = all_vars_in_chunk[i]\n",
        "                indicator2, var2 = all_vars_in_chunk[j]\n",
        "                links.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"var1\": var1['var'],\n",
        "                    \"var2\": var2['var'],\n",
        "                    \"indicator1\": indicator1,\n",
        "                    \"indicator2\": indicator2,\n",
        "                    \"response\": {\n",
        "                        \"relationship\": \"\",\n",
        "                        \"evidence\": \"\"\n",
        "                    }\n",
        "                })\n",
        "    return links\n",
        "chunk_w_vars = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "links = process_links(chunk_w_vars)\n",
        "links[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_chunks = json.load(open(\"data/v2/user/pipeline/identify_vars/chunk_w_vars.json\"))\n",
        "all_chunks = all_chunks[:10]\n",
        "candidate_links = query.filter_candidate_links(all_chunks)\n",
        "chunk_dict = {chunk['id']: dict(chunk, **{'identify_links_result':[]}) for chunk in all_chunks}\n",
        "raw_variable_definitions = json.load(open('GPTUtils/contexts/variable_definitions.json'))\n",
        "variable_definitions= {\n",
        "    var_data['var_name']: var_data['definition']\n",
        "    for var_list in raw_variable_definitions.values()\n",
        "    for var_data in var_list\n",
        "}\n",
        "\n",
        "prompt_template = json.load(open('GPTUtils/prompts/identify_links.json'))\n",
        "system_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['system_prompt_blocks']]\n",
        "user_prompt_blocks = [prompt_block[1] for prompt_block in prompt_template['user_prompt_blocks']]\n",
        "prompt_variables = {\n",
        "    \"links\": candidate_links,\n",
        "    \"variable_definitions\": variable_definitions,\n",
        "}\n",
        "prompt_list = []\n",
        "chunk_id_list = []\n",
        "link_metadata_list = []\n",
        "print(len(candidate_links))\n",
        "for link in candidate_links:\n",
        "    if link['var1'] == '其他' or link['var2'] == '其他': continue\n",
        "    conversation = chunk_dict[link['chunk_id']]['conversation']\n",
        "    prompt_variables['conversation'] = query.conversation_to_string(conversation)\n",
        "    prompt_variables['var1'] = f\"{link['var1']}, {variable_definitions[link['var1']]}\"\n",
        "    prompt_variables['var2'] = f\"{link['var2']}, {variable_definitions[link['var2']]}\"\n",
        "    prompt, response_format, extract_response_func = prompts.identify_link_prompt_factory(system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    prompt_list.append(prompt)\n",
        "    chunk_id_list.append(link['chunk_id'])\n",
        "    link_metadata_list.append(link) \n",
        "responses = query.multithread_prompts(openai_client, prompt_list, response_format=response_format, temperature=0.0)\n",
        "if response_format == 'json':\n",
        "    responses = [extract_response_func(i) for i in responses]\n",
        "responses\n",
        "for (response_index, extraction_result) in enumerate(responses):\n",
        "    if extraction_result is None: continue\n",
        "    chunk_id = chunk_id_list[response_index]\n",
        "    chunk = chunk_dict[chunk_id]\n",
        "    link_metadata = link_metadata_list[response_index]\n",
        "    chunk[\"identify_links_result\"].append({\n",
        "        \"chunk_id\": link_metadata['chunk_id'],\n",
        "        \"var1\": link_metadata['var1'],\n",
        "        \"var2\": link_metadata['var2'],\n",
        "        \"indicator1\": link_metadata['indicator1'],\n",
        "        \"indicator2\": link_metadata['indicator2'],\n",
        "        \"response\": extraction_result\n",
        "    })\n",
        "all_chunks = list(chunk_dict.values())\n",
        "save_json(all_chunks, \"data/v2/user/pipeline/identify_links/chunk_w_links.json\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "def check_evidences(results, max_index):\n",
        "    for evidences in results:\n",
        "        for evidence in evidences:\n",
        "            if evidence >= max_index:\n",
        "                return False\n",
        "    return True\n",
        "for chunk in chunks:\n",
        "    # var_type_evidences = [var_type['evidence'] for var_type in chunk['identify_var_types_result']]\n",
        "    # var_evidences = [var['evidence'] for var_list in chunk['identify_vars_result'].values() for var in var_list] \n",
        "    link_evidences = [link['response']['evidence'] for link in chunk['identify_links_result']]\n",
        "    if not check_evidences(link_evidences, len(chunk['conversation'])):\n",
        "        print(chunk['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_pairwise_jaccard(candidate_sets):\n",
        "    from itertools import combinations\n",
        "    total_jaccard_distance = 0\n",
        "    pairs = list(combinations(candidate_sets, 2))\n",
        "    for pair in pairs:\n",
        "        set1, set2 = pair\n",
        "        set1 = set(set1)\n",
        "        set2 = set(set2)\n",
        "        union = set1.union(set2)\n",
        "        if len(union) == 0: continue\n",
        "        intersection = set1.intersection(set2)\n",
        "        jaccard_distance = (len(union) - len(intersection)) / len(union)\n",
        "        total_jaccard_distance += jaccard_distance\n",
        "    return total_jaccard_distance / len(pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "def merge_var_types(var_type_results):\n",
        "    merged_var_types = {}\n",
        "    for var_types in var_type_results:\n",
        "        for var_type in var_types:\n",
        "            if var_type['var_type'] not in merged_var_types:\n",
        "                merged_var_types[var_type['var_type']] = var_type\n",
        "        else:\n",
        "            merged_var_types[var_type['var_type']]['evidence'] = list(set(merged_var_types[var_type['var_type']]['evidence'] + var_type['evidence']))\n",
        "    return list(merged_var_types.values())\n",
        "# identify var types\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append([])\n",
        "        iteration_results[chunk_index].append(chunk['identify_var_types_result'])\n",
        "for chunk_index, chunk in enumerate(all_chunks):\n",
        "    ensemble_var_types = merge_var_types(iteration_results[chunk_index])\n",
        "    candidate_var_types = list(map(lambda var_types: [x['var_type'] for x in var_types], iteration_results[chunk_index]))\n",
        "    uncertainty = average_pairwise_jaccard(candidate_var_types)\n",
        "    for var_type_result in ensemble_var_types:\n",
        "        var_type_occurrence = len(list(filter(lambda candidate: var_type_result['var_type'] in candidate, candidate_var_types)))\n",
        "        confidence = var_type_occurrence / k\n",
        "        var_type_result['uncertainty'] = uncertainty\n",
        "        var_type_result['confidence'] = confidence\n",
        "    chunk['identify_var_types_result'] = ensemble_var_types\n",
        "    if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "    chunk['uncertainty']['identify_var_types'] = uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def merge_vars(vars_results):\n",
        "    merged_vars = {}\n",
        "    for vars in vars_results:\n",
        "        for var in vars:\n",
        "            if var['var'] not in merged_vars:\n",
        "                merged_vars[var['var']] = var\n",
        "            else:\n",
        "                merged_vars[var['var']]['evidence'] = list(set(merged_vars[var['var']]['evidence'] + var['evidence']))\n",
        "    return list(merged_vars.values())\n",
        "# identify vars\n",
        "iteration_results = []\n",
        "k=5\n",
        "for _ in range(k):\n",
        "    # all_chunks = query.identify_vars(all_chunks, openai_client, system_prompt_blocks, user_prompt_blocks, prompt_variables)\n",
        "    # all_chunks = json.load(open(\"data/v2/user/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "    all_chunks = json.load(open(\"data/v2/user/pipeline/identify_links/chunk_w_links.json\"))\n",
        "    for chunk_index, chunk in enumerate(all_chunks):\n",
        "        if len(iteration_results) <= chunk_index: iteration_results.append(defaultdict(list))\n",
        "        for var_type, vars in chunk['identify_vars_result'].items():\n",
        "            # iteration_results[chunk_index][var_type].append(list(map(lambda x: x['var'], vars)))\n",
        "            iteration_results[chunk_index][var_type].append(vars)\n",
        "for chunk_index, chunk in enumerate(all_chunks[:3]):\n",
        "    for var_type, vars in iteration_results[chunk_index].items():\n",
        "        ensemble_vars = merge_vars(vars)\n",
        "        candidate_vars = list(map(lambda var_list: [x['var'] for x in var_list], vars))\n",
        "        vars_set = set([var for var_list in candidate_vars for var in var_list])\n",
        "        uncertainty = average_pairwise_jaccard(candidate_vars)\n",
        "        for var in vars_set:\n",
        "            var_occurrence = len(list(filter(lambda candidate: var in candidate, candidate_vars)))\n",
        "            confidence = var_occurrence / k\n",
        "            var_index = list(map(lambda x: x['var'], ensemble_vars)).index(var)\n",
        "            ensemble_vars[var_index]['confidence'] = confidence\n",
        "            ensemble_vars[var_index]['uncertainty'] = uncertainty\n",
        "            chunk['identify_vars_result'][var_type] = ensemble_vars\n",
        "        if 'uncertainty' not in chunk: chunk['uncertainty'] = {}\n",
        "        chunk['uncertainty']['identify_vars'] = uncertainty\n",
        "print(all_chunks[2]['identify_vars_result'])\n",
        "\n",
        "# save_json(all_chunks, \"data/v2/user/pipeline/identify_vars/chunk_w_vars_w_uncertainty.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "\n",
        "def prepare_for_finetuning(conversation, prompt):\n",
        "    conversation = merge_consecutive(conversation)\n",
        "    if len(conversation)  == 1:\n",
        "        return None\n",
        "    assert(len(conversation) > 1)\n",
        "    conversation = reformat_conversation_direct(conversation, prompt)\n",
        "    # conversations = reformat_conversation_reverse(conversation, prompt)\n",
        "    return conversation\n",
        "def merge_consecutive(conversation):\n",
        "    merged_conversation = []\n",
        "    for i, message in enumerate(conversation):\n",
        "        if i == 0:\n",
        "            merged_conversation.append(message)\n",
        "            continue\n",
        "        if conversation[i]['speaker'] == conversation[i-1]['speaker']:\n",
        "            merged_conversation[-1]['content'] += conversation[i]['content']\n",
        "        else:\n",
        "            merged_conversation.append(message)\n",
        "    return merged_conversation\n",
        "\n",
        "def reformat_conversation_direct(conversation, prompt):\n",
        "    finetune_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": prompt\n",
        "        }, \n",
        "    ]\n",
        "    for i in range(0, len(conversation), 2):\n",
        "        if i == len(conversation) - 1:\n",
        "            break\n",
        "        assert(conversation[i]['speaker'] == 1)\n",
        "        assert(conversation[i+1]['speaker'] == 0)\n",
        "        finetune_messages.append(\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conversation[i]['content']\n",
        "            })\n",
        "        finetune_messages.append(\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": conversation[i+1]['content']\n",
        "            }\n",
        "        )\n",
        "    return {\"messages\": finetune_messages}\n",
        "\n",
        "def reformat_conversations_reverse(conversation, prompt):\n",
        "    finetune_messages = []\n",
        "    for i in range(0, len(conversation), 2):\n",
        "        if i == len(conversation) - 1:\n",
        "            break\n",
        "        assert(conversation[i]['speaker'] == 1)\n",
        "        assert(conversation[i+1]['speaker'] == 0)\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": prompt\n",
        "            }, \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conversation[i]['content']\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": conversation[i+1]['content']\n",
        "            }\n",
        "        ]\n",
        "        finetune_messages.append(messages)\n",
        "    return {\"messages\": finetune_messages}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N6\n",
            "N11\n",
            "N10\n",
            "N7\n",
            "N17\n",
            "N16\n",
            "N1\n",
            "N15\n",
            "N2\n",
            "N19\n",
            "N18\n",
            "N3\n",
            "N14\n",
            "N8\n",
            "N13\n",
            "N4\n",
            "N5\n",
            "N12\n",
            "N9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "instruction_prompt = \"\"\"You are a residence of Lyudao, a small island near Taiwan. You are being interviewed about the situation on the island, ranging from environmental issues to social problems. Please answer the interview questions.\"\"\"\n",
        "for chunked_file in glob.glob(\"data/v2/user/chunk/chunk_summaries_w_ktte/*.json\"):\n",
        "    participant = chunked_file.split('/')[-1].split('.')[0]\n",
        "    print(participant)\n",
        "    chunks = json.load(open(chunked_file))\n",
        "    all_examples = []\n",
        "    for chunk in chunks:\n",
        "        conversation = chunk['conversation']\n",
        "        finetune_example = prepare_for_finetuning(conversation, instruction_prompt)\n",
        "        if finetune_example is not None:\n",
        "            all_examples.append(finetune_example)\n",
        "    Path(f\"data/finetune/{participant}\").mkdir(parents=True, exist_ok=True)\n",
        "    with open(f'data/finetune/{participant}/finetune_examples.jsonl', 'w') as outfile:\n",
        "        for example in all_examples:\n",
        "            json.dump(example, outfile, ensure_ascii=False)\n",
        "            outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a residence of Lyudao, a small island near Taiwan. You are being interviewed about the situation on the island, ranging from environmental issues to social problems. Please answer the interview questions.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"那你覺得，綠島對你來說是什麼？\"\n",
        "    }\n",
        "]\n",
        "prompts = [messages for _ in range(10)]\n",
        "# model = \"ft:gpt-4o-mini-2024-07-18:personal::9wD13Bqv\"\n",
        "participant = \"general\"\n",
        "# model = \"ft:gpt-4o-mini-2024-07-18:personal:lyudao-n5:9wFlzFxH\"\n",
        "model = \"ft:gpt-4o-mini-2024-07-18:personal:lyudao-n7:9wIU4hNG\"\n",
        "# model = \"gpt-4o-mini\"\n",
        "temperature = 0.5\n",
        "responses = query.multithread_prompts(openai_client, prompts, model=model, temperature=temperature, seed=42)\n",
        "save_json(responses, f\"data/finetune/{participant}/finetune_responses_{temperature}.json\")\n",
        "# TODO: try inserting chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 3]\n",
            "[]\n",
            "[10]\n",
            "[9, 3, 5]\n",
            "[]\n",
            "[1]\n",
            "[8, 3, 5]\n",
            "[0, 1, 3]\n",
            "[]\n",
            "[1, 10]\n",
            "[]\n",
            "[8, 1, 5, 9]\n",
            "[1]\n",
            "[]\n",
            "[1, 5]\n",
            "[5]\n",
            "[2]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1, 14]\n",
            "[]\n",
            "[1, 3, 5]\n",
            "[]\n",
            "[1, 4]\n",
            "[]\n",
            "[8, 1, 2, 3]\n",
            "[]\n",
            "[1, 4]\n",
            "[0, 2, 3]\n",
            "[1]\n",
            "[1, 2, 6]\n",
            "[3]\n",
            "[1]\n",
            "[3]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[0, 1]\n",
            "[]\n",
            "[1]\n",
            "[1]\n",
            "[]\n",
            "[1, 2, 3, 4, 7]\n",
            "[]\n",
            "[]\n",
            "[3]\n",
            "[]\n",
            "[3]\n",
            "[3]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[3]\n",
            "[1]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[1, 5]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 2]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[1, 2]\n",
            "[1]\n",
            "[10, 12]\n",
            "[]\n",
            "[3]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[1, 3]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1, 10, 3, 9]\n",
            "[4, 6]\n",
            "[1]\n",
            "[11]\n",
            "[1, 3, 12, 13, 14]\n",
            "[8, 1, 7]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[2, 3]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[8, 1, 4]\n",
            "[0, 1, 4]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[10, 11, 4]\n",
            "[1, 3, 5]\n",
            "[2, 3]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1]\n",
            "[5]\n",
            "[1]\n",
            "[1]\n",
            "[5]\n",
            "[1, 3, 7]\n",
            "[1, 3]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 2]\n",
            "[2, 5, 6]\n",
            "[]\n",
            "[0, 1]\n",
            "[]\n",
            "[]\n",
            "[12]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[49, 4, 44]\n",
            "[2]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 15]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 2]\n",
            "[1, 3]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[5, 7]\n",
            "[]\n",
            "[19, 17, 3, 7]\n",
            "[]\n",
            "[]\n",
            "[1, 3, 7]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[9, 10, 12, 6]\n",
            "[]\n",
            "[1, 3]\n",
            "[]\n",
            "[1, 3]\n",
            "[1, 9, 10, 11, 15]\n",
            "[1]\n",
            "[7]\n",
            "[3, 7]\n",
            "[3, 5]\n",
            "[2]\n",
            "[]\n",
            "[1, 3]\n",
            "[8, 3, 5]\n",
            "[]\n",
            "[11]\n",
            "[10]\n",
            "[3, 4]\n",
            "[1, 5, 9]\n",
            "[1]\n",
            "[]\n",
            "[5]\n",
            "[]\n",
            "[7]\n",
            "[11, 12, 14]\n",
            "[3]\n",
            "[3]\n",
            "[15]\n",
            "[4, 5]\n",
            "[5]\n",
            "[]\n",
            "[1, 2]\n",
            "[]\n",
            "[9, 5]\n",
            "[1, 6, 11, 14, 16]\n",
            "[0]\n",
            "[1, 5]\n",
            "[11]\n",
            "[1, 4]\n",
            "[11, 12]\n",
            "[9, 2, 3]\n",
            "[3, 5]\n",
            "[3]\n",
            "[]\n",
            "[1]\n",
            "[1, 2, 6]\n",
            "[4, 5]\n",
            "[1]\n",
            "[3]\n",
            "[1]\n",
            "[1, 3]\n",
            "[1]\n",
            "[]\n",
            "[8, 9, 5]\n",
            "[0, 1]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[12]\n",
            "[1]\n",
            "[3, 4, 15, 16, 18]\n",
            "[]\n",
            "[11, 23]\n",
            "[0, 1]\n",
            "[]\n",
            "[9, 3, 7]\n",
            "[]\n",
            "[2, 4, 6]\n",
            "[]\n",
            "[]\n",
            "[1, 3]\n",
            "[5, 8, 15, 20, 21, 26]\n",
            "[3]\n",
            "[1]\n",
            "[1]\n",
            "[1]\n",
            "[]\n",
            "[1]\n",
            "[1]\n",
            "[8, 10]\n",
            "[1]\n",
            "[0, 1]\n",
            "[1, 2, 3, 6, 7, 9]\n",
            "[1, 2]\n",
            "[1, 5, 7]\n",
            "[8, 5, 7]\n",
            "[]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[3, 4, 6, 8, 14, 16]\n",
            "[]\n",
            "[1]\n",
            "[13, 22]\n",
            "[10, 12]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[3]\n",
            "[1]\n",
            "[3]\n",
            "[]\n",
            "[9, 10, 7]\n",
            "[4]\n",
            "[1]\n",
            "[]\n",
            "[9, 10]\n",
            "[4, 6]\n",
            "[1]\n",
            "[11]\n",
            "[3, 4, 5, 6, 8, 9, 10]\n",
            "[1, 7]\n",
            "[3, 5]\n",
            "[1, 3]\n",
            "[]\n",
            "[15]\n",
            "[8, 2]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[8, 7]\n",
            "[0, 1, 4]\n",
            "[6]\n",
            "[9, 5]\n",
            "[1, 2]\n",
            "[0, 1, 3, 4, 5]\n",
            "[11, 1, 3, 4]\n",
            "[1, 5, 7, 11, 12]\n",
            "[3]\n",
            "[3, 5, 7]\n",
            "[8, 10, 11]\n",
            "[]\n",
            "[]\n",
            "[0, 1, 4, 5]\n",
            "[]\n",
            "[1]\n",
            "[3]\n",
            "[1]\n",
            "[3]\n",
            "[5]\n",
            "[5, 7]\n",
            "[3]\n",
            "[8, 21, 13]\n",
            "[8, 9]\n",
            "[3, 5, 6]\n",
            "[0, 1, 2, 3, 4, 5]\n",
            "[1, 2]\n",
            "[5, 6]\n",
            "[0, 1, 3, 5, 7, 9]\n",
            "[2, 3]\n",
            "[1, 4, 5, 6]\n",
            "[5]\n",
            "[9]\n",
            "[11]\n",
            "[1]\n",
            "[1]\n",
            "[1, 7]\n",
            "[49, 4, 44, 14]\n",
            "[1]\n",
            "[5]\n",
            "[0, 3]\n",
            "[]\n",
            "[3]\n",
            "[0, 4, 6, 12, 14, 16]\n",
            "[]\n",
            "[1, 4]\n",
            "[15]\n",
            "[]\n",
            "[1, 2]\n",
            "[]\n",
            "[1, 2, 4]\n",
            "[2, 7]\n",
            "[3]\n",
            "[]\n",
            "[1]\n",
            "[1]\n",
            "[1, 2, 5]\n",
            "[3, 5]\n",
            "[1, 5]\n",
            "[1]\n",
            "[9, 11, 13, 17, 19]\n",
            "[4]\n",
            "[1]\n",
            "[3, 4, 5, 7]\n",
            "[10, 4, 5]\n",
            "[10, 6]\n",
            "[]\n",
            "[1, 3, 7]\n",
            "[5]\n",
            "[3]\n",
            "[10, 12]\n",
            "[3]\n",
            "[1, 3]\n",
            "[1]\n",
            "[5, 15]\n",
            "[]\n",
            "[]\n",
            "[3]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[9, 5, 7]\n",
            "[]\n",
            "[7]\n",
            "[]\n",
            "[]\n",
            "[1, 5]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[2, 4]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[20]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 4]\n",
            "[]\n",
            "[8]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[6]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[2, 3, 5, 6]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[0, 1, 4, 9, 10, 12, 19]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1, 4]\n",
            "[]\n",
            "[]\n",
            "[9]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[22, 15]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 5]\n",
            "[]\n",
            "[10, 22]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[10, 12]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 3]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[4]\n",
            "[9]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[16, 19]\n",
            "[8]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[12, 14]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 11, 4]\n",
            "[32, 36, 30]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[2]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[3, 7]\n",
            "[]\n",
            "[21, 5, 6, 7]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[49, 44]\n",
            "[1]\n",
            "[]\n",
            "[5]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 3]\n",
            "[1, 5, 7]\n",
            "[]\n",
            "[1]\n",
            "[9]\n",
            "[3, 4]\n",
            "[3]\n",
            "[2]\n",
            "[]\n",
            "[1, 3]\n",
            "[8, 5]\n",
            "[]\n",
            "[11, 4, 6]\n",
            "[10]\n",
            "[3, 4]\n",
            "[8, 9]\n",
            "[1]\n",
            "[9, 11]\n",
            "[1, 5]\n",
            "[6]\n",
            "[5, 7]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[3]\n",
            "[3]\n",
            "[11]\n",
            "[2, 4, 5]\n",
            "[5]\n",
            "[1]\n",
            "[1, 2]\n",
            "[]\n",
            "[12]\n",
            "[5]\n",
            "[9, 11]\n",
            "[0, 1]\n",
            "[1, 3, 5]\n",
            "[9, 11, 12]\n",
            "[1, 4]\n",
            "[1]\n",
            "[]\n",
            "[8, 2, 3]\n",
            "[3, 5]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1, 3]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[3]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[2]\n",
            "[1]\n",
            "[9, 12]\n",
            "[1, 5]\n",
            "[8, 9, 10, 14, 19]\n",
            "[]\n",
            "[11, 3, 23]\n",
            "[]\n",
            "[]\n",
            "[1, 5]\n",
            "[]\n",
            "[1, 3, 5, 6, 12]\n",
            "[9, 7]\n",
            "[9, 3]\n",
            "[]\n",
            "[2, 4, 6]\n",
            "[]\n",
            "[]\n",
            "[3]\n",
            "[5, 8, 15, 22, 26]\n",
            "[3]\n",
            "[1]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[1, 5]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1, 3]\n",
            "[1, 9, 7]\n",
            "[]\n",
            "[1, 2]\n",
            "[]\n",
            "[3, 13, 14]\n",
            "[1]\n",
            "[1, 2, 7]\n",
            "[16, 14]\n",
            "[2]\n",
            "[1]\n",
            "[21, 22]\n",
            "[10, 12]\n",
            "[]\n",
            "[1, 5]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1, 3]\n",
            "[3]\n",
            "[3]\n",
            "[9]\n",
            "[3]\n",
            "[1, 5]\n",
            "[4]\n",
            "[]\n",
            "[]\n",
            "[9, 10]\n",
            "[4, 6]\n",
            "[1]\n",
            "[]\n",
            "[1]\n",
            "[11]\n",
            "[17, 18, 14, 15]\n",
            "[1, 7]\n",
            "[5]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[15]\n",
            "[8, 2, 4]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[8, 12, 14]\n",
            "[0, 1, 4]\n",
            "[1, 6]\n",
            "[1, 5]\n",
            "[9, 5]\n",
            "[2]\n",
            "[]\n",
            "[11, 1, 3]\n",
            "[32, 36, 5, 29, 30]\n",
            "[3, 5]\n",
            "[3]\n",
            "[3, 5, 7]\n",
            "[]\n",
            "[]\n",
            "[2]\n",
            "[0, 3, 5]\n",
            "[1]\n",
            "[]\n",
            "[1]\n",
            "[1, 3]\n",
            "[1]\n",
            "[1, 3]\n",
            "[]\n",
            "[3, 7]\n",
            "[3]\n",
            "[8, 3, 21, 7]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[2]\n",
            "[5, 6]\n",
            "[9]\n",
            "[1]\n",
            "[3]\n",
            "[1, 4, 5, 6]\n",
            "[5]\n",
            "[10]\n",
            "[8]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[49, 4, 45, 44]\n",
            "[1]\n",
            "[]\n",
            "[2, 5]\n",
            "[0, 1, 2, 3, 6, 7]\n",
            "[0, 3]\n",
            "[]\n",
            "[3]\n",
            "[0, 4, 6, 12, 14]\n",
            "[]\n",
            "[4]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[1]\n",
            "[2, 3]\n",
            "[2, 5]\n",
            "[3]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[1]\n",
            "[]\n",
            "[8, 5]\n",
            "[1]\n",
            "[0, 1]\n",
            "[]\n",
            "[1, 4]\n",
            "[17, 19, 7]\n",
            "[4]\n",
            "[]\n",
            "[4, 5, 7]\n",
            "[3]\n",
            "[]\n",
            "[]\n",
            "[1, 3, 9, 7]\n",
            "[]\n",
            "[]\n",
            "[10]\n",
            "[]\n",
            "[1, 3]\n",
            "[1, 3]\n",
            "[15]\n",
            "[1]\n",
            "[3]\n",
            "[4, 5]\n",
            "[1]\n",
            "[3]\n",
            "[7]\n",
            "[9, 10, 15]\n",
            "[1]\n",
            "[10]\n",
            "[]\n",
            "[9, 3, 5]\n",
            "[1]\n",
            "[2, 3]\n",
            "[]\n",
            "[]\n",
            "[2, 4]\n",
            "[]\n",
            "[]\n",
            "[1, 10]\n",
            "[3, 4]\n",
            "[8, 3, 4]\n",
            "[1]\n",
            "[]\n",
            "[3, 4, 5]\n",
            "[6]\n",
            "[5, 7]\n",
            "[11, 14]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[1, 3]\n",
            "[]\n",
            "[5, 7]\n",
            "[]\n",
            "[]\n",
            "[2]\n",
            "[]\n",
            "[9, 12, 5, 1]\n",
            "[9, 10]\n",
            "[14]\n",
            "[]\n",
            "[0, 1, 3, 5]\n",
            "[9, 13, 7]\n",
            "[1]\n",
            "[4]\n",
            "[]\n",
            "[17, 18, 13, 15]\n",
            "[8, 11]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[1, 4]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[3]\n",
            "[1]\n",
            "[]\n",
            "[10, 12]\n",
            "[5]\n",
            "[8, 16, 14, 7]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[4, 5]\n",
            "[]\n",
            "[]\n",
            "[9, 7]\n",
            "[9]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[9, 7]\n",
            "[]\n",
            "[28, 30]\n",
            "[3]\n",
            "[1]\n",
            "[]\n",
            "[]\n",
            "[4, 5]\n",
            "[1, 5]\n",
            "[]\n",
            "[1, 8, 10, 18, 20, 22]\n",
            "[]\n",
            "[1]\n",
            "[3, 4]\n",
            "[]\n",
            "[0, 1]\n",
            "[2]\n",
            "[]\n",
            "[9, 5, 7]\n",
            "[]\n",
            "[2, 3, 4, 5]\n",
            "[20]\n",
            "[1, 2]\n",
            "[1]\n",
            "[21, 22, 23]\n",
            "[10, 12]\n",
            "[1, 4]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1, 4]\n",
            "[]\n",
            "[9, 10]\n",
            "[5]\n",
            "[1, 5]\n",
            "[]\n",
            "[]\n",
            "[1, 3]\n",
            "[9, 3]\n",
            "[4, 6]\n",
            "[1]\n",
            "[1]\n",
            "[9]\n",
            "[1, 2]\n",
            "[]\n",
            "[11, 13]\n",
            "[14, 15, 16, 17, 18]\n",
            "[4]\n",
            "[7]\n",
            "[1]\n",
            "[3]\n",
            "[]\n",
            "[]\n",
            "[8, 3, 4]\n",
            "[3]\n",
            "[1, 2]\n",
            "[]\n",
            "[]\n",
            "[8]\n",
            "[4]\n",
            "[]\n",
            "[1, 6]\n",
            "[9, 10]\n",
            "[]\n",
            "[9, 3, 5, 6]\n",
            "[11, 3, 13]\n",
            "[0, 1, 3, 30]\n",
            "[12, 13]\n",
            "[3]\n",
            "[6]\n",
            "[7]\n",
            "[8, 10, 12]\n",
            "[9]\n",
            "[8, 1, 2, 3]\n",
            "[9]\n",
            "[1, 3, 4, 5]\n",
            "[]\n",
            "[1, 2]\n",
            "[2]\n",
            "[1, 4]\n",
            "[3]\n",
            "[0, 1]\n",
            "[3]\n",
            "[]\n",
            "[5, 7]\n",
            "[3, 7]\n",
            "[3]\n",
            "[8, 10, 18]\n",
            "[9, 11]\n",
            "[1, 3]\n",
            "[]\n",
            "[2]\n",
            "[4, 5, 6]\n",
            "[9]\n",
            "[1]\n",
            "[3]\n",
            "[6, 7]\n",
            "[1, 2, 6, 7]\n",
            "[]\n",
            "[]\n",
            "[10, 3, 14]\n",
            "[]\n",
            "[]\n",
            "[6]\n",
            "[1, 10, 21, 14]\n",
            "[]\n",
            "[1]\n",
            "[1]\n",
            "[6, 7]\n",
            "[3]\n",
            "[1, 2]\n",
            "[3]\n",
            "[16, 6]\n",
            "[9]\n",
            "[1, 5]\n",
            "[1, 3]\n",
            "[]\n",
            "[15]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1]\n",
            "[2, 5]\n",
            "[2, 4]\n",
            "[16, 12, 13, 15]\n",
            "[22]\n",
            "[]\n",
            "[1]\n",
            "[5]\n",
            "[8, 1, 3, 5]\n",
            "[1]\n",
            "[]\n",
            "[1, 3]\n",
            "[5]\n",
            "[]\n",
            "[4]\n",
            "[15]\n",
            "[4]\n",
            "[1, 2]\n",
            "[]\n",
            "[5]\n",
            "[4, 6]\n",
            "[10, 12, 7]\n",
            "[9, 11, 12]\n",
            "[5]\n",
            "[]\n",
            "[12]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from DataUtils import dr\n",
        "from collections import defaultdict\n",
        "import jieba\n",
        "\n",
        "\n",
        "def generate_DPSIR_data(chunks, var_type_mentions, variable_definitions, keyword_embeddings, stopwords=[], userdict=None):\n",
        "    chunks_dict = {chunk['id']: chunk for chunk in chunks}\n",
        "    for var_type, variables in variable_definitions.items():\n",
        "        variables = {var_data['var_name']: var_data for var_data in variables}\n",
        "        variable_definitions[var_type] = variables\n",
        "    res = {}\n",
        "    mentioned_chunk_ids = set()\n",
        "    for var_type, var_type_data in var_type_mentions.items():\n",
        "        res[var_type] = {\n",
        "            \"variable_type\": var_type,\n",
        "            \"variable_mentions\": {},\n",
        "            \"keyword_data\": {\n",
        "                \"keyword_list\": [],\n",
        "                \"keyword_coordinates\": {},\n",
        "                \"keyword_statistics\": {}\n",
        "            }\n",
        "        }\n",
        "        for variable_data in var_type_data['variable_mentions'].values():\n",
        "            variable_name = variable_data['variable_name']\n",
        "            if variable_name not in res[var_type]['variable_mentions']:\n",
        "                res[var_type]['variable_mentions'][variable_name] = {\n",
        "                    \"variable_name\": variable_name,\n",
        "                    \"definition\": variable_definitions[var_type][variable_name]['definition'] if variable_name in variable_definitions[var_type] else \"unknown\",\n",
        "                    \"factor_type\": variable_definitions[var_type][variable_name]['factor_type'] if variable_name in variable_definitions[var_type] else \"unknown\",\n",
        "                    \"mentions\": [],\n",
        "                }\n",
        "            res[var_type]['variable_mentions'][variable_name]['mentions'] += variable_data['mentions']\n",
        "            mentioned_chunk_ids.update(list(map(lambda x: x['chunk_id'], variable_data['mentions'])))\n",
        "        mentioned_chunk_data = [chunks_dict[chunk_id] for chunk_id in mentioned_chunk_ids]\n",
        "        keyword_list, keyword_statistics, keyword_coordinates = generate_keyword_data(var_type, mentioned_chunk_data, keyword_embeddings, stopwords, userdict)\n",
        "        res[var_type]['keyword_data'] = {\n",
        "            \"keyword_list\": keyword_list,\n",
        "            \"keyword_statistics\": keyword_statistics,\n",
        "            \"keyword_coordinates\": keyword_coordinates\n",
        "        }\n",
        "    return res\n",
        "\n",
        "def generate_keyword_data(var_type, chunks, keyword_embeddings, stopwords=[], userdict=None):\n",
        "\n",
        "    keyword_embeddings_dict = {keyword['keyword']: keyword for keyword in keyword_embeddings}\n",
        "    all_keywords = set()\n",
        "    keyword_statistics = defaultdict(int)\n",
        "    keyword_coordinates = {}\n",
        "    for chunk in chunks:\n",
        "        if chunk['identify_var_types_result'] == []: continue\n",
        "        evidences = list(set([sentence_index for evidences in map(lambda x: x['evidence'], filter(lambda y: y['var_type'] == var_type, chunk['identify_var_types_result'])) for sentence_index in evidences]))\n",
        "        print(evidences)\n",
        "        evidence_messages = [chunk['conversation'][i]['content'] for i in evidences]\n",
        "        chunk_keywords = set()\n",
        "        for sentence in evidence_messages:\n",
        "            words = jieba.cut(sentence)\n",
        "            words = list(filter(lambda x: x not in stopwords, words))\n",
        "            words = list(filter(lambda x: x in keyword_embeddings_dict, words))\n",
        "            chunk_keywords.update(words)\n",
        "        for keyword in chunk_keywords:\n",
        "            keyword_statistics[keyword] += 1\n",
        "        all_keywords.update(chunk_keywords)\n",
        "    all_keywords = list(all_keywords) \n",
        "    keyword_statistics = {\n",
        "        keyword: {\n",
        "            \"frequency\": freq\n",
        "            } \n",
        "        for keyword, freq in keyword_statistics.items()\n",
        "    }\n",
        "    # all_keyword_embeddings = [keyword_embeddings_dict[keyword]['embedding'] for keyword in all_keywords] \n",
        "    # XY = dr.scatter_plot(all_keyword_embeddings)\n",
        "    # for keyword, coordinate in zip(all_keywords, XY):\n",
        "    #     keyword_coordinates[keyword] = coordinate.tolist()\n",
        "    return all_keywords, keyword_statistics, keyword_coordinates\n",
        "\n",
        "nodes = {}\n",
        "var_types = [\"driver\", \"pressure\", \"state\", \"impact\", \"response\"]\n",
        "node_data_path = \"data/v2/user/nodes/\"\n",
        "for var_type in var_types:\n",
        "    nodes[var_type] = json.load(\n",
        "        open(node_data_path + f\"{var_type}_nodes.json\", encoding=\"utf-8\")\n",
        "    )\n",
        "variable_definitions = json.load(\n",
        "    open(\"GPTUtils/contexts/variable_definitions.json\", encoding=\"utf-8\")\n",
        ")\n",
        "chunks = json.load(open(\"data/v2/user/pipeline/identify_var_types/chunk_w_var_types.json\"))\n",
        "keyword_embeddings = json.load(open(\"data/v2/user/keyword/keywords.json\"))\n",
        "stopwords = [\"綠島\"]\n",
        "userdict = \"data/v2/user/keyword/userdict.txt\"\n",
        "res = generate_DPSIR_data(chunks, nodes, variable_definitions, keyword_embeddings, stopwords, userdict)\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords = json.load(open(\"data/v2/user/keyword/keywords.json\"))\n",
        "keyword_list = [keyword['keyword'] for keyword in keywords]\n",
        "# write keyword to a txt file, each line is a keyword\n",
        "with open(\"data/v2/user/keyword/userdict.txt\", \"w\") as f:\n",
        "    for keyword in keyword_list:\n",
        "        f.write(f\"{keyword}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keyword_embeddings = json.load(open(\"data/v2/user/keyword/keywords.json\"))\n",
        "kpca_reducer = dr.init_kpca_reducer(\n",
        "    list(map(lambda x: x[\"embedding\"], keyword_embeddings))\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lyudao",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
